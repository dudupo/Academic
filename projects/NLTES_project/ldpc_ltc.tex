\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[a4paper, total={7.5in, 10in} ]{geometry}
\usepackage{braket}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{svg}
\usepackage{float}
\usepackage{tikz}
\usetikzlibrary{patterns,shapes.arrows}
\usepackage{adjustbox}
\usepackage{tikz-network}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{multicol}
\usepackage[backend=biber,style=alphabetic,sorting=ynt]{biblatex}
\usepackage{xcolor}
\usepackage{pgfplots}
\DeclareUnicodeCharacter{2212}{âˆ’}
\usepgfplotslibrary{groupplots,dateplot}
\pgfplotsset{compat=newest}

\addbibresource{sample.bib} %Import the bibliography file

\newcommand{\commentt}[1]{\textcolor{blue}{ \textbf{[COMMENT]} #1}}
\newcommand{\ctt}[1]{\commentt{#1}}
\newcommand{\prb}[1]{ \mathbf{Pr} \left[ {#1} \right]}
\newcommand{\onotation}[1]{\(\mathcal{O} \left( {#1}  \right) \)}
\newcommand{\ona}[1]{\onotation{#1}}
\newcommand{\PSI}{{\ket{\psi}}}
\newcommand{\LESn}{\ket{\psi_n}}
\newcommand{\LESa}{\ket{\phi_n}}
\newcommand{\LESs}{\frac{1}{\sqrt{n}}\sum_{i}{\ket{\left(0^{i}10^{n-i}\right)^{n}}}}
\newcommand{\Hn}{\mathcal{H}_{n}}
\newcommand{\Ep}{\frac{1}{\sqrt{2^n}}\sum^{2^n}_{x}{ \ket{xx}}}
\newcommand{\HON}{\ket{\psi_{\text{honest}}}}
\newcommand{\Lemma}{\paragraph{Lemma.}}
\newcommand{\PonB}{ \rho + \frac{5}{16}\delta\le \frac{3}{4} + \frac{1}{16} } 
\newcommand{\Cpa}{[n, \rho n, \delta n]}
%\setlength{\columnsep}{0.6cm}

\newcommand{\Gz}{ G_{z}^{\delta} } 
\newcommand{ \Tann } {  \mathcal{T}\left( G, C_0 \right) }
\begin{document}


\title{Simple Almost LTC Good LDPC Codes} 
\author{David Ponarovsky}
\maketitle
\abstract{We propose a new simple construction based on Tanner Codes, which yields a good LDPC code with testability query complexity of $\Theta\left( n^{1-\varepsilon} \right)$ for any $\varepsilon> 0$ .} 
\begin{multicols*}{2}
  \section{Preambles}
  In this work, we propose a new construction for good LDPC codes, which also have a good testability parameter. In the sense that one can engineer the code such that doing $\Theta\left( n^{1-\varepsilon}\right)$ random checks, at most, would be enough to detect any error with probability proportional to its size. When $\varepsilon$ is a small, as we wish, fixed positive number.  In contrast to previews, constructions made by C and D, our construction does not involve any amplification and is almost identical to the expander code construction, which is considered by many simple and easy to implement. We remark that Dinur, in her previous work \cite{Dinur}, obtained a better testability code with only constant query complexity. 
  Yet, her construction relies on the existence of a specific square complex. That complex, which is also an essential ingredient of the Quantum good LDPC proof, allows engineering a restrictions system with high degeneration. This complex is also necessary to provide a structure of commuting Pauli cheks which is needed to define a quantum CSS code \cite{Pavel}, \cite{leverrier2022quantum}. That fact raises the question: Could the preview constructions also provide such degeneration? What is the exact relation between Quantemness and Testability? Could one create a testability code without having a quantum code? We partially answer those questions above by showing that a slight adaptation of the Tanner-Expander code provides effective, as we wish, testability.
Our proof also indirectly answers the following question. Why most of the good LDPC codes are known to be bad in terms of detecting errors? In other words, It seems that for most of them, there exist strings that are very far from being in the code and, meanwhile, fail to satisfy only a small number of restrictions.
While the previous LDPC constructions focused on ensuring that the yielded code would have a good rate and distance parameters, our construction enforces the restrictions collection to have a nontrivial fraction of degeneration. That is, removing a single restriction will not change the code, as any restriction is linearly dependent on the others.



 \section{Introduction}
  Coding theory has emerged by the need to transfer information in noisy communication channels. By embedding a message in higher dimension space, one can guarantee robustness against possible faults. The ratio of the original content length to the passed message length is the rate of the code, and it measures how consuming our communication protocol is. Furthermore, the distance of the code quantifies how many faults the scheme can absorb such that the receiver could recover the original message. Also, We could think about the code as all the strings that satisfy a specified restrictions collection.
 Non-formally, we say that code is good if its distance and rate are scaled linearly in the encoded message length.  In practice, one might also intrested to be able to implement those checks efficiently. We say that a code is an LDPC if any bit is involved in a constant number of restrictions, each of which is a linear equation, and if any restriction contains a fixed number of variables. And finally, another characteristic of the code is its testability, which is the complexity of the number of random checks one should be done in order to negate that a given candidate is in the code.
Besides the fact that good codes are considered efficient in terms of robustness and overhead, they are also vital components in establishing secure multiparty computation \cite{MultiParty} and have a deep connection to probabilistic proofs.

First, we state the notations, definitions, and formal theorem in section 2. Then in sections 3 and 4, we review past results and provide their proofs in order to make this paper self-contained. Readers familiar with the basic concepts of LDPC, Tanner and Expanders codes construction should consider skipping directly to section 5, in which we provide our proof. 
%Linear Error Correction Codes, 
\subsection{Notations, Definitions, And Our Contribution}
Here we focus only on linear binary codes, which one could think about as linear subspaces of $\mathbb{F}_{2}^{n}$. A common way to measure resilience is to ask how many bits an evil entity needs to flip such that the corrupted vector will be closer to another vector in that space than the original one. Those ideas were formulated by Hamming \cite{Hamming}, who presented the following definitions. 
\paragraph{Definition.} Let $n \in \mathbb{N}$ and $\rho, \delta\in \left( 0,1 \right)$. We say that $C$ is a \textit{binary linear code} with parameters $[n, \rho n, \delta n]$. If $C$ is a subspace of $\mathbb{F}_{2}^{n}$, and the dimension of $C$ is at least $\rho n$. In addition, we call the vectors belong to $C$ \textit{codewords} and define the distance of $C$ to be the minimal number of different bits between any codewords pair of $C$.   

From now on, we will use the term code to refer to linear binary codes, as we don't deal with any other types of codes. Also, even though it is customary to use the above parameters to analyze codes, we will use their percent forms called the relative distance and the rate of code, matching $\delta$ and $\rho$ correspondingly.     
\paragraph{Definition.} A \textit{family of codes} is an infinite series of codes. Additionally, suppose the rates and relative distances converge into constant values $\rho,\delta$. In that case, we abuse the notation and call that family of codes a code with $[n, \rho n, \delta n]$ for fixed $\rho, \delta\in [ 0,1 )$, and infinite integers $n \in \mathbb{N}$.     

Notice that the above definition contains codes with parameters attending to zero. From a practical view, it means that etiher we send too many bits, more than a constant amount, on each bit in the original message. Or that for big enough $n$, adversarial, limited to changing only a constant fraction of the bits, could disrupt the transmission. That distinction raises the definition of good codes.

\paragraph{Definition.}We will say that a family of codes is a \textit{good code} if its parameters converge into positive values. 

Apart from distance and rate here, we interest also that the checking process will be robust. In particular,  we wish that against significant errors, forgetting to perform a single check will sabotage the computation only with a tiny probability.  
\paragraph{Definition.} Consider a code $C$  a string $x$, and denote by $\xi\left( x \right)$ the fraction of the checks in which $x$ fails. $C$ will be called a \textit{local-testability $f\left( n \right)$} If there exsits $\kappa > 0$ such that 
\begin{equation*}
  \begin{split}
    \frac{d\left( x, C \right)}{n} \le \kappa \cdot  \xi\left( x \right) f\left( n \right)
  \end{split}
\end{equation*}



Nowadays, we are aware of a wide range of constructions yield good codes, including the expander codes of Sipser and Spilman \cite{ExpanderCodes} and the LTC codes of Dinur \cite{Dinur}, \cite{Pavel}, \cite{leverrier2022quantum}. Thus if a decade ago, the main question was the existence of a good code and its construction, now, and particularly in this work, we concentrate on getting a deep understanding of what makes those constructions work. By utilizing those insights, we seccussed in achieving a significantly simpler both constructions and their correctness proof. Our results: 

\paragraph{Theorem:} For every $\varepsilon > 0 $ there is a family of good LDPC codes with $\Theta\left( n^{1-\varepsilon} \right)$ testability.     

\subsection{Singleton Bound}  
To get a feeling of the behavior of the distance-rate trade-of, Let us consider the following two codes; each demonstrates a different extreme case. First, define the repetition code $C_{r} \subset \mathbb{F}_{2}^{n \cdot r}$, In which, for a fixed integer $r$, any bit of the original string is duplicated $r$ times. Second, consider the parity check code $C_{p} \subset \mathbb{F}_{2}^{n+1}$, which it's codewords are only the vectors with even parity. Let us analyze the repetition code. Clearly, any two $n$-bits different messages must have at least a single different bit. Therefore their corresponding encoded codewords have to differ in at least $r$ bits. Hence, by scaling $r$, one could achieve a higher distance as he wishes. Sadly the rate of the code decays as $n/nr = 1/r$. In contrast, the parity check code adds only a single extra bit for the original message. Therefore scaling $n$ gives a family which has a rate attends to $\rho \rightarrow 1$. However, flipping any two different bits of a valid codeword is conversing the parity and, as a result, leads to another valid codeword.

To summarize the above, we have that, using a simple construction, one could construct the codes $[r, 1, r]$, $[r, r-1, 2]$. Each has a single perfect parameter while the other decays to the worst. In the next section, we will review the Singleton bound, which states that for any code (not necessarily good), there must be a zero-sum game between the relative distance and the rate.
Now, we are ready to formulate our contribution. 


Besides of been the first bound, Singleton bound demonstrates how one could get results by using relatively simple elementary arguments. It is also engaging to ask why the proof yields a bound that, empirically, seems far from being tight.
\paragraph{Theorem, Singleton Bound.} For any linear code with parameter $[n,k,d]$, the following inequality holds:
\begin{equation*}
    k+ d \le n + 1
\end{equation*} 
\textbf{Proof:} Since any two codewords of $C$ differ by at least $d$ coordinates, we know that by ignoring the first $d-1$ coordinate of any vector, we obtain a new code with one-to-one corresponding to the original code. In other words, we have found a new code with the same dimension embedded in $\mathbb{F}_{2}^{n-d+1}$. Combine the fact that dimension is, at most, the dimension of the container space, we get that:  
\begin{equation*}
  \begin{split}
    \dim C &= 2^{k} \le 2^{n-d+1} \Rightarrow k+d \le n + 1
  \end{split}
\end{equation*}
$\square$

It is also well known that the only binary codes that reach the bound are: $[n,1,n]$, $[n,n-1,2]$,$[n,n,1]$ \cite{eczoo_mds}. In particular, there are no good binary codes that obtain equality. Next, we will review Tanner's construction, that in addition to being a critical element to our proof, also serves as an example of how one can construct a code with arbitrary length and positive rate.
\subsection{Tanner Code}
The constructions require two main ingredients: a graph $\Gamma$, and for simplicity, we will restrict ourselves to a $\Delta$ regular graph. Secondly, a small code $C_{0}$ at length equals the graph's regularity, namely $C_{0} = [\Delta,\rho\Delta, \delta\Delta]$. We can think about any bit string at length E as an assignment over the edges of the graph. Furthermore, for every vertex $v \in \Gamma$, we will call the bit string, which is set on its edges, the local view of $v$.  Then we can define, \cite{Tanner}:
\paragraph{Definition.}  Let $ C = \mathcal{T}\left( \Gamma, C_{0} \right)$  be all the codewords which, for any vertex $v\in \Gamma$, the local view of $v$ is a codeword of $C_{0}$. We say that $C$ is a Tanner code of $\Gamma, C_{0}$. Notice that if $C_{0}$ is a binary linear code, So $C$ is.  


It's also worth mentioning that the first construction of good classical codes, due to Sipser and Shpilman, are Tanner codes over expanders graphs \cite{ExpanderCodes}.
\paragraph{Theorem} Tanner codes have a rate of at least $2\rho - 1$.
\textbf{Proof:}  The dimension of the subspace is bounded by the dimension of the container minus the number of restrictions. So assuming non-degeneration of the small code restrictions, we have that any vertex count exactly $ \left( 1 - \rho  \right)\Delta $ restrictions. Hence, \begin{equation*}
  \begin{split}
    \dim C & \ge \frac{1}{2}n\Delta - \left( 1-\rho \right)\Delta n = \frac{1}{2}n\Delta\left( 2\rho - 1 \right)  
  \end{split}
\end{equation*} Clearly, any small code with rate $> \frac{1}{2}$ will yield a code with an asymptotically positive rate $\square$ 
\subsection{Expander Codes}
We saw how a graph could give us arbitrarily long codes with a positive rate. We will show, Sipser's result that if the graph is also an expander, we can guarantee a positive relative distance. We notice that the name expander codes is coined for a more general version than the one we will present.   
\paragraph{Definition.} Denote by $\lambda$ the second eigenvalue of the adjacency matrix of the $\Delta$-regular graph. For our uses, it will be satisfied to define expander as a graph $G = \left( V,E \right)$ such that for any two subsets of vertices $T,S \subset V$, the number of edges between $S$ and $T$ is at most:
\begin{equation*}
  \begin{split}
    \mid E\left( S,T \right) - \frac{\Delta}{n}|S||T| \mid \le \lambda\sqrt{|S| |T|} 
  \end{split}
\end{equation*}
This bound is known as the Expander Mixining Lemma. 
\paragraph{Theorem.} Theorem, let $C$ be the Tanner Code defined by the small code $C_{0} = [\Delta,\delta\Delta, \rho\Delta ]$ such that $\rho \ge \frac{1}{2}$ and the expander graph $G$ such that $\delta\Delta \ge \lambda$. $C$ is a good  LDPC code.
\paragraph{Proof.} We have already shown that the graph has a positive rate due to the Tunner construction. So it's left to show also the code has a linear distance. Fix a codeword $x \in C$ and denote By $S$ the support of $x$ over the edges. Namely, a vertex $v\in V$ belongs to $S$ if it connects to nonzero edges regarding the assignment by $x$, Assume towards contradiction that $|x| = o\left( n \right)$. And notice that $|S|$ is at most $2|x|$, Then by The Expander Mixining Lemma we have that: 
\begin{equation*}
  \begin{split}
    \frac{E\left( S,S \right)}{|S|} & \le \frac{\Delta}{n}|S|  + \lambda \\
    & \le_{ n \rightarrow \infty} o\left( 1 \right) + \lambda
  \end{split}
\end{equation*}
Namely, for any such, sublinear weight, $x$, the average of nontrivial edges for the vertex is less than $\lambda$. So there must be at least one vertex $v \in S$ that, on his local view, sets a  string at a weight less than $\lambda$. By the definition of $S$, this string cannot be a trivial string. Combining the fact that any nontrivial codeword of the $C_{0}$ is at weight at least $\delta\Delta$, we get a contradiction to the assumption that $v$ is satisfied, videlicet, $x$ can't be a codeword $\square$
\subsection{Tanner testability.} This subsection will explain why testability is so hard to achieve. Let $C$ be a good Tanner expander code as defined above. And consider an arbitrary vertex $u \in V$ and arbitrary restriction of $C_{0}$, $h$. Now define $\tilde{C}$ as the code obtained by requiring all the restrictions of $C$ except $h$ on $u$. That it, $u$ is satisfied if his local view satisfies all the $C_{0}$ restrictions apart from $h$.
Also, for convenience, denote the small code $u$ enforces on his local view by $C_{0}^{u}$. Let us assume that the distance of $C_{0}^{u}$ is at least $\delta\Delta$. 
Then, by repeating almost exactly the steps above with caution, one could prove that $\tilde{C}$ also has a linear distance. 


Assume that $\tilde{C} \neq C \Rightarrow$ there exists $x\in \tilde{C} / C$. By definition, for any $v\in V / \{u\}$ it holdes that $x|_{v} \in C_{0}$. Hence, the assumption that $ x \notin C$  implies $x|_{u} \notin C_{0}$ So, clearly, $x$ fails at a constant number of $C$'s checks. On the other hand, the closest codeword $y \in C$ to $x$ is also a codeword of $\tilde{C}$ as $y|_{v} \in C_{0}$ for every $v\in V$.  Hence:
\begin{equation*}
  \begin{split}
    d\left( x,C \right) &= d\left( x,y \right) \ge d\left( \tilde{C} \right) =\Theta\left( n \right)
  \end{split}
\end{equation*}
That is, even that a linear number of bits are needed to be flipped to correct $x$, only a single check observes that $x$ is indeed an error.
  \section{Construction}
\subsection{ Almost LTC With Zero Rate}
\paragraph{Definition. The Disagreement Code.} Given a Tanner code $C = \Tann$, define the code $C_{\oplus}$ to contain all the words equal to the formal summation $ \sum_{v \in V\left( G \right)} {c_{v} }$ when $c_{v}$ is an assignment of a codeword $ c_{v} \in C_0 $  on the edges of the vertex $ v \in V\left( G \right)$.
We call to such code the \textit{disagreement code} of $C$, as edges are set to 1 only if their connected vertices contribute to the summation codewords that are different on the corresponding bit to that edge. In addition, we will call to any contribute $c_v$, the \textit{suggestion} of $v$. And notice that by linearity, each vertex suggests at most a single suggestion.   
 
\paragraph{Theorem 1.} For every $\varepsilon > 0$, there exist $\Delta_{\varepsilon}\in \mathbb{N}$ and $\alpha>0$ such that if $\Delta \ge \Delta_{\varepsilon}$, then for every $x \in C_{\oplus}$ at Hamming weight at most $\alpha |E|^{1-\varepsilon}$, there exists a vertex $v \in V$ and a small codeword $c_{v} \in C_{0} $ such that adding the assignment of it over the $v$'s edges to $x$ define the codeword $x + c_{v}$  such that $|x + c_{v}| < |x|$.  

\paragraph{Proof.} By induction over the number of vertices $V^\prime \subset V$, which suggest a nontrivial codeword to $x$. Base, assume that there is just a single vertex $v \in V$ that suggests a nontrivial codeword $c_{v} \in C_{0}$ . Then it's clear that $x = c_{v}$. And therefore, we have that $|x +c_{v}| = 0 < |x|$.

Assume the correctness of the argument for every codeword defined by at most $m$ nontrivial suggestions made by $V^\prime \subset V$.  And consider the graph $\left( V^\prime, E^\prime \right)$ induced by them. If the graph has more than a single connectivity component, then any of them is also a codeword of $C_{\oplus}$  but composed by at most $m-1$ nontrivial suggestions. Therefore, by the assumption, we could find a vertex $v$ and a proper small codeword $c_v \in C_0 $, such that the addition of the suggestion will decrease the weight of the codeword defined on that component and therefore decrease the total weight of $x$.

So, we can assume that the vertices in $V^\prime$ compose a single connectivity component. Let be $x|_{v} \in \mathbb{F}_{2}^{\Delta}$ the bits of $x$ on the indices corresponding to $v$'s edges. If there is any $v$, with suggestion $c_{v}$, such that $ \frac{1}{2}w\left( c_{v}\right) < w\left( x|_{v} \right)$, then we could pick to turn on $c_{v}$ again and have that:
\begin{equation*}
  \begin{split}
    |x+c_{v}| & = | x_{/v} + x_{v} + c_{v}| = |x_{/v}| + w\left( c_{v} \right) + w\left( x|_{v} \right) \\
    & < |x_{/v}| + \frac{1}{2}w\left( c_{v} \right) \le |x_{/v}| + w\left( x|_{v} \right) = |x|
  \end{split}
\end{equation*}

Hence it is left to consider the case that for any $v\in V^\prime$, it holds that $\frac{1}{2}w\left( c_{v}\right) >  w\left( x|_{v} \right)$ (Notice that if they equal, then by turn on $c_{v}$, we back again to codeword made by $m-1$ nontrivial suggestions). We will prove that this case is possible only for codewords with wight at least $\alpha|E|^{1-\varepsilon}$.

For any $S \subset E$, define $w_{S}\left( x \right)$ to be the weight that $x$ induces over $S$. And notice that any edge of $E$ connected only to a single vertex in $V^\prime$ equals the corresponding bit in the original suggestion made by $c_{v}$. Hence for every $v\in V^\prime$, it holds that $w_{E / E^\prime}\left(x|_{v}\right) = w_{E / E^\prime}\left(c_{v}\right)$. 
\paragraph{Claim.} For any $v \in V^\prime$ and corresponded suggestion $c_{v}$ it holds that: $w_{E^\prime}\left( c_{v} \right) \ge \frac{1}{2}\delta_{0}\Delta$. 
\paragraph{Proof:}By using the previews insight we get: \begin{equation*}
  \begin{split}
    w_{E^\prime}\left( c_{v} \right) &= w\left( c_{v} \right) - w_{E / E^\prime}\left( c_{v} \right) =  w\left( c_{v} \right) - w_{E / E^\prime}\left( x|_{v} \right) \\ 
     & \ge  w\left( c_{v} \right) - w\left( x|_{v} \right) \ge \frac{1}{2}w\left( c_{v} \right) = \frac{1}{2}\delta_{0}\Delta 
  \end{split}
\end{equation*}
$\square$

Consider an arbitrary vertex $r \in V^\prime$, and consider the DAG obtained by the BFS walk over the subgraph $\left(V^\prime, E^\prime \right)$ starting at $r$. Denote this directed tree by $T$.
Let $g$ be the girth of the graph, and consider a layer $U$ in $T$ at height $h\left( U \right)$ satisfies the inequality $ h\left( U \right) < \frac{1}{2}g + l$ for some integer $l$.
  %\begin{adjustbox}{width=150pt}%\columnwidth}
\begin{figure*}[t]%{width=150pt} %0.3\textwidth}
	\begin{tikzpicture}
\clip (0,0) rectangle (7.0,15.0);
\Vertex[x=2.738,y=11.761,size=0.1,color=blue]{0}
\Vertex[x=2.998,y=10.093,size=0.1,color=blue]{119}
\Vertex[x=4.071,y=11.888,size=0.1,color=blue]{1}
\Vertex[x=2.063,y=11.127,size=0.1,color=blue]{2}
\Vertex[x=6.615,y=5.488,size=0.1]{3}
\Vertex[x=2.110,y=10.165,size=0.1,color=blue]{64}
\Vertex[x=2.087,y=5.928,size=0.1]{4}
\Vertex[x=2.681,y=5.829,size=0.1]{9}
\Vertex[x=2.725,y=4.777,size=0.1]{5}
\Vertex[x=2.832,y=3.143,size=0.1]{16}
\Vertex[x=5.240,y=5.092,size=0.1]{6}
\Vertex[x=4.072,y=5.571,size=0.1]{25}
\Vertex[x=2.324,y=5.044,size=0.1]{7}
\Vertex[x=2.634,y=3.781,size=0.1]{96}
\Vertex[x=6.114,y=2.782,size=0.1]{8}
\Vertex[x=4.720,y=10.313,size=0.1,color=blue]{49}
\Vertex[x=1.273,y=5.235,size=0.1]{10}
\Vertex[x=2.907,y=4.806,size=0.1]{81}
\Vertex[x=2.423,y=11.059,size=0.1,color=blue]{11}
\Vertex[x=5.240,y=6.917,size=0.1]{40}
\Vertex[x=3.142,y=3.394,size=0.1]{12}
\Vertex[x=3.362,y=6.184,size=0.1]{13}
\Vertex[x=5.001,y=3.625,size=0.1]{24}
\Vertex[x=6.453,y=3.322,size=0.1]{14}
\Vertex[x=3.715,y=5.024,size=0.1]{15}
\Vertex[x=3.448,y=6.590,size=0.1]{105}
\Vertex[x=1.665,y=4.177,size=0.1]{17}
\Vertex[x=3.042,y=4.018,size=0.1]{18}
\Vertex[x=1.564,y=11.184,size=0.1,color=blue]{19}
\Vertex[x=2.443,y=4.312,size=0.1]{20}
\Vertex[x=2.565,y=6.669,size=0.1]{21}
\Vertex[x=0.880,y=3.656,size=0.1]{22}
\Vertex[x=0.532,y=3.797,size=0.1]{23}
\Vertex[x=3.016,y=4.935,size=0.1]{26}
\Vertex[x=5.272,y=3.485,size=0.1]{27}
\Vertex[x=5.850,y=5.696,size=0.1]{28}
\Vertex[x=2.602,y=10.657,size=0.1,color=blue]{29}
\Vertex[x=2.989,y=12.675,size=0.1,color=blue]{30}
\Vertex[x=3.872,y=10.629,size=0.1,color=blue]{31}
\Vertex[x=0.755,y=2.491,size=0.1]{32}
\Vertex[x=6.816,y=5.640,size=0.1]{33}
\Vertex[x=5.381,y=6.258,size=0.1]{34}
\Vertex[x=5.914,y=4.103,size=0.1]{35}
\Vertex[x=0.452,y=6.785,size=0.1]{36}
\Vertex[x=4.753,y=4.448,size=0.1]{37}
\Vertex[x=3.008,y=2.917,size=0.1]{38}
\Vertex[x=1.685,y=3.783,size=0.1]{39}
\Vertex[x=3.718,y=11.312,size=0.1,color=blue]{41}
\Vertex[x=2.442,y=5.347,size=0.1]{42}
\Vertex[x=2.260,y=5.836,size=0.1]{43}
\Vertex[x=6.475,y=2.571,size=0.1]{44}
\Vertex[x=0.662,y=3.408,size=0.1]{45}
\Vertex[x=0.744,y=5.813,size=0.1]{46}
\Vertex[x=0.210,y=4.238,size=0.1]{47}
\Vertex[x=4.213,y=6.110,size=0.1]{48}
\Vertex[x=4.163,y=5.517,size=0.1]{50}
\Vertex[x=4.851,y=5.208,size=0.1]{51}
\Vertex[x=6.114,y=4.959,size=0.1]{52}
\Vertex[x=5.850,y=3.092,size=0.1]{53}
\Vertex[x=3.680,y=2.507,size=0.1]{54}
\Vertex[x=5.991,y=6.548,size=0.1]{55}
\Vertex[x=6.900,y=3.262,size=0.1]{56}
\Vertex[x=0.269,y=3.221,size=0.1]{57}
\Vertex[x=2.328,y=3.838,size=0.1]{58}
\Vertex[x=1.362,y=10.371,size=0.1,color=blue]{59}
\Vertex[x=3.236,y=12.459,size=0.1,color=blue]{60}
\Vertex[x=1.088,y=10.688,size=0.1,color=blue]{61}
\Vertex[x=0.637,y=5.593,size=0.1]{62}
\Vertex[x=2.225,y=5.644,size=0.1]{63}
\Vertex[x=3.419,y=3.743,size=0.1]{65}
\Vertex[x=2.486,y=4.826,size=0.1]{66}
\Vertex[x=2.521,y=5.843,size=0.1]{67}
\Vertex[x=5.250,y=3.397,size=0.1]{68}
\Vertex[x=2.328,y=6.213,size=0.1]{69}
\Vertex[x=4.180,y=3.006,size=0.1]{70}
\Vertex[x=2.808,y=10.597,size=0.1,color=blue]{71}
\Vertex[x=5.644,y=5.416,size=0.1]{72}
\Vertex[x=4.837,y=4.869,size=0.1]{73}
\Vertex[x=3.638,y=5.182,size=0.1]{74}
\Vertex[x=4.437,y=3.054,size=0.1]{75}
\Vertex[x=4.880,y=2.617,size=0.1]{76}
\Vertex[x=0.665,y=4.855,size=0.1]{77}
\Vertex[x=4.547,y=4.414,size=0.1]{78}
\Vertex[x=5.472,y=10.701,size=0.1,color=blue]{79}
\Vertex[x=2.640,y=2.532,size=0.1]{80}
\Vertex[x=1.813,y=3.322,size=0.1]{82}
\Vertex[x=2.475,y=3.430,size=0.1]{83}
\Vertex[x=0.913,y=3.804,size=0.1]{84}
\Vertex[x=0.286,y=6.393,size=0.1]{85}
\Vertex[x=6.800,y=4.979,size=0.1]{86}
\Vertex[x=3.312,y=6.797,size=0.1]{87}
\Vertex[x=0.100,y=3.986,size=0.1]{88}
\Vertex[x=5.420,y=10.021,size=0.1,color=blue]{89}
\Vertex[x=3.035,y=10.415,size=0.1,color=blue]{90}
\Vertex[x=5.166,y=10.021,size=0.1,color=blue]{91}
\Vertex[x=0.310,y=4.306,size=0.1]{92}
\Vertex[x=5.925,y=4.589,size=0.1]{93}
\Vertex[x=6.046,y=3.116,size=0.1]{94}
\Vertex[x=2.911,y=2.544,size=0.1]{95}
\Vertex[x=0.657,y=5.937,size=0.1]{97}
\Vertex[x=5.953,y=5.754,size=0.1]{98}
\Vertex[x=0.244,y=2.572,size=0.1]{99}
\Vertex[x=2.043,y=4.341,size=0.1]{100}
\Vertex[x=2.173,y=10.021,size=0.1,color=blue]{101}
\Vertex[x=6.722,y=2.335,size=0.1]{102}
\Vertex[x=2.386,y=5.617,size=0.1]{103}
\Vertex[x=6.790,y=5.698,size=0.1]{104}
\Vertex[x=3.708,y=5.700,size=0.1]{106}
\Vertex[x=6.491,y=3.419,size=0.1]{107}
\Vertex[x=1.111,y=5.632,size=0.1]{108}
\Vertex[x=4.336,y=10.021,size=0.1,color=blue]{109}
\Vertex[x=5.675,y=6.449,size=0.1]{110}
\Vertex[x=1.693,y=3.265,size=0.1]{111}
\Vertex[x=1.525,y=3.532,size=0.1]{112}
\Vertex[x=0.381,y=2.325,size=0.1]{113}
\Vertex[x=0.142,y=2.377,size=0.1]{114}
\Vertex[x=4.848,y=4.673,size=0.1]{115}
\Vertex[x=4.756,y=4.823,size=0.1]{116}
\Vertex[x=2.587,y=11.125,size=0.1,color=blue]{117}
\Vertex[x=2.477,y=11.813,size=0.1,color=blue]{118}
\Edge[,color=blue,opacity=1](0)(119)
\Edge[,color=blue,opacity=0.2](0)(1)
\Edge[,color=blue,opacity=0.2](0)(0)
\Edge[,color=black,opacity=1](0)(30)
\Edge[,color=black,opacity=1](0)(60)
\Edge[,color=blue,opacity=1](0)(90)
\Edge[,color=black,opacity=1](119)(118)
\Edge[,color=blue,opacity=1](119)(1)
\Edge[,color=blue,opacity=1](1)(2)
\Edge[,color=blue,opacity=0.2](1)(1)
\Edge[,color=blue,opacity=1](1)(11)
\Edge[,color=black,opacity=1](1)(19)
\Edge[,color=blue,opacity=1](1)(29)
\Edge[,color=blue,opacity=1](1)(31)
\Edge[,color=black,opacity=1](1)(41)
\Edge[,color=black,opacity=1](1)(49)
\Edge[,color=black,opacity=1](1)(59)
\Edge[,color=blue,opacity=1](1)(61)
\Edge[,color=blue,opacity=1](1)(71)
\Edge[,color=blue,opacity=1](1)(79)
\Edge[,color=blue,opacity=1](1)(89)
\Edge[,color=black,opacity=1](1)(91)
\Edge[,color=black,opacity=1](1)(101)
\Edge[,color=black,opacity=1](1)(109)
\Edge[,color=blue,opacity=0.2](2)(3)
\Edge[,opacity=0.2](2)(64)
\Edge[,opacity=0.2](3)(4)
\Edge[,opacity=0.2](3)(9)
\Edge[,opacity=0.2](64)(8)
\Edge[,color=blue,opacity=0.2](64)(22)
\Edge[,color=blue,opacity=0.2](64)(28)
\Edge[,opacity=0.2](64)(32)
\Edge[,opacity=0.2](64)(38)
\Edge[,opacity=0.2](64)(52)
\Edge[,opacity=0.2](64)(58)
\Edge[,opacity=0.2](64)(62)
\Edge[,opacity=0.2](64)(63)
\Edge[,color=blue,opacity=0.2](64)(65)
\Edge[,opacity=0.2](64)(16)
\Edge[,color=blue,opacity=0.2](64)(68)
\Edge[,color=blue,opacity=0.2](64)(82)
\Edge[,opacity=0.2](64)(88)
\Edge[,opacity=0.2](64)(92)
\Edge[,opacity=0.2](64)(98)
\Edge[,color=blue,opacity=0.2](64)(112)
\Edge[,color=black,opacity=1](64)(118)
\Edge[,opacity=0.2](4)(5)
\Edge[,opacity=0.2](4)(16)
\Edge[,opacity=0.2](9)(8)
\Edge[,opacity=0.2](9)(10)
\Edge[,opacity=0.2](9)(81)
\Edge[,opacity=0.2](9)(27)
\Edge[,opacity=0.2](9)(33)
\Edge[,opacity=0.2](9)(57)
\Edge[,opacity=0.2](9)(63)
\Edge[,opacity=0.2](9)(87)
\Edge[,opacity=0.2](9)(93)
\Edge[,opacity=0.2](9)(117)
\Edge[,opacity=0.2](5)(6)
\Edge[,opacity=0.2](5)(25)
\Edge[,opacity=0.2](16)(14)
\Edge[,opacity=0.2](16)(15)
\Edge[,opacity=0.2](16)(17)
\Edge[,opacity=0.2](16)(16)
\Edge[,opacity=0.2](16)(26)
\Edge[,opacity=0.2](16)(34)
\Edge[,opacity=0.2](16)(44)
\Edge[,opacity=0.2](16)(46)
\Edge[,opacity=0.2](16)(56)
\Edge[,opacity=0.2](16)(74)
\Edge[,opacity=0.2](16)(76)
\Edge[,opacity=0.2](16)(86)
\Edge[,opacity=0.2](16)(94)
\Edge[,opacity=0.2](16)(104)
\Edge[,opacity=0.2](16)(106)
\Edge[,opacity=0.2](16)(116)
\Edge[,opacity=0.2](6)(7)
\Edge[,opacity=0.2](6)(96)
\Edge[,opacity=0.2](25)(24)
\Edge[,opacity=0.2](25)(26)
\Edge[,opacity=0.2](25)(25)
\Edge[,opacity=0.2](25)(35)
\Edge[,opacity=0.2](25)(55)
\Edge[,opacity=0.2](25)(65)
\Edge[,opacity=0.2](25)(85)
\Edge[,opacity=0.2](25)(95)
\Edge[,opacity=0.2](25)(115)
\Edge[,opacity=0.2](7)(8)
\Edge[,opacity=0.2](7)(49)
\Edge[,opacity=0.2](96)(24)
\Edge[,opacity=0.2](96)(36)
\Edge[,opacity=0.2](96)(54)
\Edge[,opacity=0.2](96)(66)
\Edge[,opacity=0.2](96)(84)
\Edge[,opacity=0.2](96)(95)
\Edge[,opacity=0.2](96)(97)
\Edge[,opacity=0.2](96)(96)
\Edge[,opacity=0.2](96)(114)
\Edge[,color=blue,opacity=0.2](49)(13)
\Edge[,color=blue,opacity=0.2](49)(17)
\Edge[,color=blue,opacity=0.2](49)(23)
\Edge[,opacity=0.2](49)(37)
\Edge[,color=blue,opacity=0.2](49)(43)
\Edge[,color=blue,opacity=0.2](49)(47)
\Edge[,opacity=0.2](49)(48)
\Edge[,color=blue,opacity=0.2](49)(50)
\Edge[,color=blue,opacity=0.2](49)(53)
\Edge[,opacity=0.2](49)(67)
\Edge[,color=blue,opacity=0.2](49)(73)
\Edge[,color=blue,opacity=0.2](49)(77)
\Edge[,opacity=0.2](49)(83)
\Edge[,color=blue,opacity=0.2](49)(97)
\Edge[,opacity=0.2](49)(103)
\Edge[,opacity=0.2](49)(107)
\Edge[,opacity=0.2](49)(113)
\Edge[,opacity=0.2](10)(11)
\Edge[,opacity=0.2](10)(40)
\Edge[,opacity=0.2](81)(21)
\Edge[,opacity=0.2](81)(39)
\Edge[,opacity=0.2](81)(51)
\Edge[,opacity=0.2](81)(69)
\Edge[,opacity=0.2](81)(80)
\Edge[,opacity=0.2](81)(82)
\Edge[,opacity=0.2](81)(81)
\Edge[,opacity=0.2](81)(99)
\Edge[,opacity=0.2](81)(111)
\Edge[,opacity=0.2](11)(12)
\Edge[,opacity=0.2](40)(20)
\Edge[,opacity=0.2](40)(39)
\Edge[,opacity=0.2](40)(41)
\Edge[,opacity=0.2](40)(40)
\Edge[,opacity=0.2](40)(50)
\Edge[,opacity=0.2](40)(70)
\Edge[,opacity=0.2](40)(80)
\Edge[,opacity=0.2](40)(100)
\Edge[,opacity=0.2](40)(110)
\Edge[,opacity=0.2](12)(13)
\Edge[,opacity=0.2](12)(24)
\Edge[,opacity=0.2](13)(14)
\Edge[,opacity=0.2](24)(18)
\Edge[,opacity=0.2](24)(23)
\Edge[,opacity=0.2](24)(42)
\Edge[,opacity=0.2](24)(48)
\Edge[,opacity=0.2](24)(72)
\Edge[,opacity=0.2](24)(78)
\Edge[,opacity=0.2](24)(102)
\Edge[,opacity=0.2](24)(108)
\Edge[,opacity=0.2](14)(15)
\Edge[,opacity=0.2](15)(105)
\Edge[,opacity=0.2](105)(45)
\Edge[,opacity=0.2](105)(75)
\Edge[,opacity=0.2](105)(104)
\Edge[,opacity=0.2](105)(106)
\Edge[,opacity=0.2](105)(105)
\Edge[,opacity=0.2](17)(18)
\Edge[,opacity=0.2](18)(19)
\Edge[,color=blue,opacity=0.2](19)(20)
\Edge[,opacity=0.2](20)(21)
\Edge[,opacity=0.2](21)(22)
\Edge[,opacity=0.2](22)(23)
\Edge[,opacity=0.2](26)(27)
\Edge[,opacity=0.2](27)(28)
\Edge[,opacity=0.2](28)(29)
\Edge[,opacity=0.2](29)(30)
\Edge[,opacity=0.2](30)(31)
\Edge[,color=blue,opacity=0.2](31)(32)
\Edge[,opacity=0.2](32)(33)
\Edge[,opacity=0.2](33)(34)
\Edge[,opacity=0.2](34)(35)
\Edge[,opacity=0.2](35)(36)
\Edge[,opacity=0.2](36)(37)
\Edge[,opacity=0.2](37)(38)
\Edge[,opacity=0.2](38)(39)
\Edge[,color=blue,opacity=0.2](41)(42)
\Edge[,opacity=0.2](42)(43)
\Edge[,opacity=0.2](43)(44)
\Edge[,opacity=0.2](44)(45)
\Edge[,opacity=0.2](45)(46)
\Edge[,opacity=0.2](46)(47)
\Edge[,opacity=0.2](47)(48)
\Edge[,opacity=0.2](50)(51)
\Edge[,opacity=0.2](51)(52)
\Edge[,opacity=0.2](52)(53)
\Edge[,opacity=0.2](53)(54)
\Edge[,opacity=0.2](54)(55)
\Edge[,opacity=0.2](55)(56)
\Edge[,opacity=0.2](56)(57)
\Edge[,opacity=0.2](57)(58)
\Edge[,opacity=0.2](58)(59)
\Edge[,opacity=0.2](59)(60)
\Edge[,color=blue,opacity=0.2](60)(61)
\Edge[,color=blue,opacity=0.2](61)(62)
\Edge[,opacity=0.2](62)(63)
\Edge[,opacity=0.2](65)(66)
\Edge[,opacity=0.2](66)(67)
\Edge[,opacity=0.2](67)(68)
\Edge[,opacity=0.2](68)(69)
\Edge[,opacity=0.2](69)(70)
\Edge[,opacity=0.2](70)(71)
\Edge[,opacity=0.2](71)(72)
\Edge[,opacity=0.2](72)(73)
\Edge[,opacity=0.2](73)(74)
\Edge[,opacity=0.2](74)(75)
\Edge[,opacity=0.2](75)(76)
\Edge[,opacity=0.2](76)(77)
\Edge[,opacity=0.2](77)(78)
\Edge[,opacity=0.2](78)(79)
\Edge[,opacity=0.2](79)(80)
\Edge[,opacity=0.2](82)(83)
\Edge[,opacity=0.2](83)(84)
\Edge[,opacity=0.2](84)(85)
\Edge[,opacity=0.2](85)(86)
\Edge[,opacity=0.2](86)(87)
\Edge[,opacity=0.2](87)(88)
\Edge[,opacity=0.2](88)(89)
\Edge[,color=blue,opacity=0.2](89)(90)
\Edge[,opacity=0.2](90)(91)
\Edge[,color=blue,opacity=0.2](91)(92)
\Edge[,opacity=0.2](92)(93)
\Edge[,opacity=0.2](93)(94)
\Edge[,opacity=0.2](94)(95)
\Edge[,opacity=0.2](97)(98)
\Edge[,opacity=0.2](98)(99)
\Edge[,opacity=0.2](99)(100)
\Edge[,opacity=0.2](100)(101)
\Edge[,color=blue,opacity=0.2](101)(102)
\Edge[,opacity=0.2](102)(103)
\Edge[,opacity=0.2](103)(104)
\Edge[,opacity=0.2](106)(107)
\Edge[,opacity=0.2](107)(108)
\Edge[,opacity=0.2](108)(109)
\Edge[,opacity=0.2](109)(110)
\Edge[,opacity=0.2](110)(111)
\Edge[,opacity=0.2](111)(112)
\Edge[,opacity=0.2](112)(113)
\Edge[,opacity=0.2](113)(114)
\Edge[,opacity=0.2](114)(115)
\Edge[,opacity=0.2](115)(116)
\Edge[,opacity=0.2](116)(117)
\Edge[,color=black,opacity=1](117)(118)
\end{tikzpicture}
%\input{network2.tex}
\end{figure*}
%\end{adjustbox} 
\paragraph{Claim.}. The number of vertices with more than a single parent in $T$ is at most ${\Delta^l}\choose{2}$.  
\paragraph{Proof:}Assume, towards contradiction, that there are more than $ \Delta^l \choose 2 $ in $U$ that has at least two parents, And consider all the subtrees of $T$ whose roots are placed in height $l$. There are, at most $\Delta^l$, such subtrees. Hence, by the assumption, there must be a pair of vertices in $U$ with parents in the same subtree. Because each subtree has a height at most half of $g$, there is a path connecting those parents at length at most $g$. Then, one can concatenate that path with the corresponding vertices in $U$ and get a cycle shorter than the girth. $\square$
\begin{figure*}[t]
\input{tree.tex}
\caption{This is a tiger.}
\end{figure*}
\paragraph{Lemma}. Let $U$ be a layer at height $l$ and denote by $ \omega = \frac{1}{2}\delta_{0}\Delta$. The number of vertices in $U$ is at least: 
\begin{equation*}
  \begin{split}
    \left( \frac{1}{2}\delta_0\Delta \right)^{l}- \frac{\left( \frac{\Delta^{2}}{\omega} \right)\Delta^{2l-g} - \omega^{l-g/2} }{\frac{\Delta^{2}}{\omega}-1}
  \end{split}
\end{equation*}

\paragraph{Proof:} We will think of $T$ as the tree obtained by purning a tree, with degree greater than $\frac{1}{2}\delta_{0}\Delta$, at height $l$, namely any vertex at height $i \le l $ with two parents can be counted as subtree with height $l-i$ that has pruned. Then the problem is equivalence to bounding from above the number of pruned vertices at height $l$. Define $X_{i}$ to be the number of pruned subtrees whose root is at height $i$. Then consider the next maximization problem: 
\begin{equation*}
  \begin{split}
    X_{i} & \le \Delta^{2\max \left\{ 0, l-g/2 \right\}} \\ 
    f & \le \sum_{g/2}^{l}{\omega^{l-i}X_{i} } \le \omega^{l-g/2}\sum_{0}^{l-g/2}{\omega^{-i}\Delta^{2i} } = \\ & \omega^{l-g/2}\frac{\left( \frac{\Delta^{2}}{\omega} \right)^{l-g/2+1}-1}{\frac{\Delta^{2}}{\omega}-1} =  
      \frac{\left( \frac{\Delta^{2}}{\omega} \right)\Delta^{2l-g} - \omega^{l-g/2} }{\frac{\Delta^{2}}{\omega}-1}
  \end{split}
\end{equation*}
By using the inequality $ |U| \ge \omega^{l} - f $ we get the bound $\square$ 
\paragraph{Claim.} there exists a constant $\Delta^{\prime}$ that depends only on $\delta_{0}$, such that if $\Delta \ge \Delta^{\prime}$, then the number number of vertices at height $\frac{3}{4}g$ is at least $ \frac{1}{2}\omega^{\frac{3}{4}g} $.

\paragraph{Proof:} The above lower bound yields: 
\begin{equation*}
  \begin{split}
    |U| \ge \omega^{\frac{3}{4}g} -  \frac{\left( \frac{\Delta^{2}}{\omega} \right)\Delta^{\frac{1}{2}g}  }{\frac{\Delta^{2}}{\omega}-1} \ge \omega^{\frac{3}{4}g} -  2 \Delta^{\frac{1}{2}g}  
  \end{split}
\end{equation*}
So it is enough to require that: $\left( \frac{1}{2}\delta_{0}\Delta  \right)^{\frac{3}{4}g} \ge 4 \Delta^{\frac{1}{2}g}$ :  
\begin{equation*}
  \begin{split}
    \Delta^{\frac{1}{4}g}\ge 4\cdot \left( \frac{2}{\delta_{0}} \right)^{\frac{3}{4}g} \Rightarrow\Delta \ge  4^{\frac{4}{g}} \left( \frac{2}{\delta_{0}} \right)^{3} 
  \end{split}
\end{equation*} So any $\Delta$ satisfies $\Delta> \left( \frac{2}{\delta_0} \right)^{3}$ is also satisfying the above inequality, and therefore for such $\Delta$-regular graphs, the size of $U$ will be at least $\frac{1}{2}\omega^{\frac{3}{4}g}$ $\square$   

\paragraph{Claim.} Assume that $ g \ge \frac{4}{3} \log_{\Delta -1 }\left( n \right) $, Then we have that for every $\varepsilon >0 $,  there exist $\Delta_{\varepsilon}$  such for every $\Delta \ge \Delta_{\varepsilon}$  it holds that $ \omega^{\frac{3}{4}g} \ge n^{1-\varepsilon}$.
\paragraph{Proof:}

\begin{equation*}
  \begin{split}
    \omega^{\frac{3}{4}g}&\ge \omega^{\log_{\Delta-1}\left( n \right)} = n^{\frac{1}{\log_{\frac{1}{2}\delta_{0}\Delta}\left( \Delta -1 \right)}} \\ 
    & \log_{\frac{1}{2}\delta_{0}\Delta}\left( \Delta -1 \right) = \frac{\ln\left( \Delta-1 \right)}{\ln\frac{1}{2}\delta_{0}+ \ln \Delta} \rightarrow_{\Delta\rightarrow \infty} 1
  \end{split}
\end{equation*} So for every $\varepsilon >0 $ there exist $\Delta_{\varepsilon}$ such that for $\Delta > \Delta_{\varepsilon}$ we get: $\omega^{\frac{3}{4}g} \ge n^{\frac{1}{1-\varepsilon}}$, and for any $\varepsilon < 1$ it holds that: $ n^{\frac{1}{1-\varepsilon}}\ge n^{1-\varepsilon}$ $\square$

Denote by $x|_{U}$ the bits of $x\in C$ correspond to the edges that connected to at least one vertex in $U$. And denote by $w_{E/E^{\prime}}\left( x|_{U} \right)$ the weight induced by the $x|_{U}$ over the edges in $E/E^\prime$.
\paragraph{Claim.} Suppose that $G$ is an expander graph with a second eigenvalue $\lambda$, then there exists a layer $U$ at the size at least $\omega^{ \frac{3}{4}g }$ such that:
\begin{equation*}
  \begin{split}
    w_{E/E^{\prime}}\left( x|_{U} \right)\ge\Delta|U|\left( \delta_{0}-\frac{2}{3}-\frac{2\lambda}{\Delta} \right)
  \end{split}
\end{equation*}
\paragraph{Proof:} Consider layer $U$ at height $\frac{3}{4}g$ and denote by $U_{-1}$ and $U_{+1}$ the preceding and the following layers to $U$ in $T$. It follows from the expander mixing lemma that:
\begin{equation*}
  \begin{split}
    w_{E/E^{\prime}}\left( x|_{U} \right) & \ge \delta_{0}\Delta|U| -w\left( E(U_{-1} \bigcup U_{+1} ,U)  \right) \ge \\ 
    & \delta_{0}\Delta|U| -\left( E(U_{-1} \bigcup U_{+1} ,U)  \right) \\ 
    &  \delta_{0}\Delta|U| - \Delta\frac{|U||U_{-1}|}{n} - \Delta\frac{|U||U_{+1}|}{n} \\
    & -\lambda\sqrt{|U||U_{-1}|} - \lambda\sqrt{|U||U_{+1}|}
  \end{split}
\end{equation*}

\paragraph{Claim.} We can assume that $|U| \ge |U_{-1}|, |U_{+1}|$. 
\paragraph{Proof:} Suppose that $|U_{+1}| > |U|$ then clearly $|U_{+1}| > \frac{1}{2}\omega^{\frac{3}{4}g} $, so we could choose $U$ to be $U_{+1}$. Continuing stepping deeper till we have that $|U| > |U_{+1}|, |U_{-1}|$. Simiraly, if $|U| > |U_{+1}|$ but $|U_{-1}| > |U|$, the we could take steps upward by replacing $U_{-1}$ with $U$. At the end of the process, we will be left with $U$ such that $|U| > \frac{1}{2}\omega^{\frac{4}{3}g}$ and $|U| > |U_{+1}|, |U_{-1}|$ $\square$

Using the the claim, we have that $\left( |U_{+1}| + |U_{-1}| \right)/n <\frac{2}{3} $ and therefore:
\begin{equation*}
  \begin{split}
    w_{E/E^{\prime}}\left( x|_{U} \right) & \ge \Delta\left( \delta_{0} - \frac{2}{3} - \frac{2\lambda}{\Delta} \right)|U| \ \  \square 
  \end{split}
\end{equation*}
That immediately yields the following: 
\begin{equation*}
  \begin{split}
    |x| \ge  w_{E/E^{\prime}}\left( x|_{U} \right) \ge \left( \delta_{0} - \frac{2}{3} - \frac{2\lambda}{\Delta} \right)\frac{1}{2}n^{1-\varepsilon} 
  \end{split}
\end{equation*}
So for any $\delta_0 \ge \frac{2}{3} + \frac{2\lambda}{\Delta} $ we have proved Theorem 1 $\square$
Unfortunately, Singelton bound doesn't allow both $\delta_0 > \frac{2}{3}$ and $\rho_0 \ge \frac{1}{2}$, so in total, we prove the existence of code LDPC code which is good in terms of testability and distance yet has a zero rate. In the next subsection, we will prove that one can overcome this problem by requiring only half of the vertices to restrict their local view to be codewords of high relative distance. 
\subsection{ Almost LTC With Positive Rate } 
To overcome the vanishing rate issue, let us restrict ourselves to using only even expanders graph. Now, Let $C_{0}^{-}$ and $C_{0}^{+}$ be two small codes with parameters $C_{0}^{\pm} = [\Delta, \rho_{\pm}\Delta, \delta_{\mp}\Delta]$. We will refer to $C_{0}^{+}$ as the positive and $C_{0}^{-}$ as the negative codes. Similarly, we will call the positive vertices to the left side of $G$ and the negative vertices to the right side of $G$ and use the notation $G = \left( V_{-}, V_{+}, E \right)$, when $V = V_{-} \cup V_{+}$. 
Consider the code $C$, which consists of all the words $x\in C$ such that $x|_{v} \in C_{0}^{\pm}$ for any $v \in V_{\pm}$.
\paragraph{Claim.} If $\rho_{-} + \rho_{+} > 1$, $\delta_{\pm}\Delta > \lambda$ and $|V_{-}| = |V_{+}| = \frac{1}{2}n$ then the $C$ has positive rate, and relative distance. 
\paragraph{Proof.} First, let us bound the rate, 
\begin{equation*}
  \begin{split}
    \dim C & \ge \frac{1}{2}\Delta n - \left( 1 - \rho_{+} \right)\Delta|V_{+}| - \left( 1 - \rho_{-} \right)\Delta|V_{-}| \\
    & \frac{1}{2}\Delta n \left( \rho_{-} + \rho_{+}  - 1  \right) 
  \end{split}
\end{equation*}
For proving a linear distance, fix a codeword $x \in C$ and denote By $S_{\pm}$ the support of $x$ over the edges. Namely, a vertex $v\in V_{\pm}$ belongs to $S_{\pm}$ if it connects to nonzero edges regarding the assignment by $x$, Assume towards contradiction that $|x| = o\left( n \right)$ . By The Expander Mixining Lemma we have that: 
\begin{equation*}
  \begin{split}
    \frac{E\left( S_{\pm}, S_{\mp} \right)}{|S_{\pm}|}\le\frac{\Delta}{n}|S_{\mp}| + \lambda\sqrt{\frac{|S_{\mp}|}{|S_{\pm}|}}
  \end{split}
\end{equation*}
Notice $|x| < |S_{-}| + |S_{+}| < 2|x|$ and $|S_{\pm}| < \Delta |S_{\mp}|$, so if $S_{-}$ is sublinear, then $S_{+}$ is also sublinear. In addition, we can assume, without loss of generality, that $|S_{-}| < |S_{+}|$. Hence, we have that the average of nontrivial incoming edges to a vertex in $|S_{+}|$ is less than $ \Delta \frac{|S_{-}|}{n} + \lambda $. That implies that for any $x = o\left( n \right)$ there must be at least a single vertex $ v \in S_{+} $ with less than $ \lambda $ nontrivial incoming edges; in other words, the weight of the word seen by $v$ is less than $\lambda$, which contradicts the assumption that $x\in C$ $\square$

\paragraph{Theorem 1+.} For every $\varepsilon > 0$, there exist $\Delta_{\varepsilon}\in \mathbb{N}$ and $\alpha>0$ such that if $\Delta \ge \Delta_{\varepsilon}$, then for every $x \in C_{\oplus}$ at Hamming weight at most $\alpha |E|^{1-\varepsilon}$, there exists a vertex $v \in V_{\pm}$ and a small codeword $c_{v} \in C_{0}^{\pm} $ such that adding the assignment of it over the $v$'s edges to $x$ define the codeword $x + c_{v}$  such that $|x + c_{v}| < |x|$.  

\paragraph{Proof:} Define the $T$ precisely as is in the proof of Theorem 1, And assume, without loss of generality $\delta_{-} < \delta_{+}$. Hence it is clear that for any $v \in V^\prime$ it holdes that $w_{E^\prime}\left( c_{v} \right) > \frac{1}{2}\delta_{-}\Delta$, so we could repeat exactly the above steps to get that there exists a layer $U \subset T$ at height $\frac{3}{4}g$ such that $ |U| = \Omega\left( n^{1-\varepsilon} \right)$ for large enough $\Delta$. Furthermore: 
\begin{equation*}
  \begin{split}
    w_{E/E^{\prime}}\left( x|_{U\cup U_{+1}} \right) & \ge \delta_{-}\Delta|U|+ \delta_{+}\Delta|U_{+1}|  -w\left( E( U_{+1} ,U)  \right) - \\ & \ \ \  w \left( E( U, U_{-1}) \right) - w \left( E( U_{+1}, U_{+2} ) \right)  \ge \\ & \ \ \ \delta_{-}\Delta|U|+ \delta_{+}\Delta|U_{+1}| - \\  & \ \ \  \frac{\Delta}{n}\left( |U||U_{-1}| + |U||U_{+1}| + |U_{+1}||U_{+2}| \right) - \\ &   \lambda \left(  \sqrt{|U||U_{-1}|} + \sqrt{|U||U_{+1}|} + \sqrt{|U_{+1}||U_{+2}|} \right)  
  \end{split}
\end{equation*}
\paragraph{Claim.}  
\begin{equation*}
  \begin{split}
     |U||U_{-1}| + |U||U_{+1}| + |U_{+1}||U_{+2}| & \le \left( |U| + |U_{+1}| \right)\frac{3}{4}n 
  \end{split}
\end{equation*}
\paragraph{Proof:} Recall that $ |U_{-1}| + |U_{+1}| =  |U_{-1} \cup U_{+1} | \le | V_{\pm} | = \frac{1}{2}n $. Then:  
\begin{equation*}
  \begin{split}
    & |U||U_{-1}| + |U||U_{+1}| + |U_{+1}||U_{+2}|  = \\
    & \  \frac{1}{2}\left( |U|(|U_{-1}| + |U_{+1}|) + |U_{+1}||U_{+2}|  \right) + \\ 
    & \  \frac{1}{2}\left( |U||U_{-1}| + |U_{+1}|(|U| + |U_{+2}|)  \right) \le \\ 
    & \ \frac{1}{2}\left( |U_{-1}||U| + |U_{+1}||U_{+2}|  \right) + \frac{1}{2}n\left( |U| +|U_{+1}| \right) \\ 
    & \ \le \left( \frac{1}{4}n + \frac{1}{2}n \right) \left( |U| + |U_{+1}|  \right) \square 
  \end{split}
\end{equation*}
Also, we could again assume that $|U| \ge |U_{-1}|,|U_{+1}|,|U_{+2}|$ and then:
\begin{equation*}
  \begin{split}
    \sqrt{|U_{i}||U_{j}|} & = \frac{\sqrt{|U_{i}||U_{j}|}}{|U|+|U_{+1}|}\cdot \left( |U| + |U_{+1}| \right) \le \\ &   \frac{\sqrt{|U_{i}||U_{j}|}}{|U|}\cdot \left( |U| + |U_{+1}| \right)  
	 \le \left( |U| + |U_{+1}| \right)
  \end{split}
\end{equation*}
So in total, we have that: 
\begin{equation*}
  \begin{split}
    w_{E/E^{\prime}}\left( x|_{U\cup U_{+1}} \right) \ge   \left( \delta_{-}+\delta_{+} - \frac{3}{4} - \frac{4\lambda}{\Delta} \right)\Delta\left( |U|+|U_{+1}| \right)
  \end{split}
\end{equation*}
And the fact that $|x| \ge w_{E/E^{\prime}}\left( x|_{U\cup U_{+1}} \right) $ proves Theorem 1+ $\square$

\subsection{ Tanner Code Testability For Small ($n^{\frac{2}{3}-\varepsilon}$) Errors}
Another side result obtained by the proof of Theorem 1 is that one cannot hide errors in a tree at a length less than half of the girth. Namely, any codeword of the disagreement code with a weight less than $ \omega^{\frac{1}{2}g} $ could be reduced, regardless of the parameters of the small code. 
\paragraph{Theorem 1-.} For every $\delta_{0}, \varepsilon > 0$, there exist $\Delta_{\varepsilon}\in \mathbb{N}$ and $\alpha>0$ such that if $\Delta \ge \Delta_{\varepsilon}$, then for every $x \in C_{\oplus}$ at Hamming weight at most $\alpha |E|^{\frac{g}{2}-\varepsilon}$, there exists a vertex $v \in V$ and a small codeword $c_{v} \in C_{0} $ such that adding the assignment of it over the $v$'s edges to $x$ define the codeword $x + c_{v}$  such that $|x + c_{v}| < |x|$.  
\paragraph{Proof:} Proof,  Define $T$ again to be the DAG constructed as in the proof of Theorem 1 and denote by $U$ the last layer of $T$. As the height of the tree is less than $\frac{1}{2}g$, then it follows from the claim that any vertex $v$ in $U$ has at most one parent in $T$, and therefore:    
\begin{equation*}
  \begin{split}
    w_{E/E^{\prime}}\left( x|_{v} \right) \ge w(c_{v}) - 1 \ge \frac{1}{2}w(c_{v})
  \end{split}
\end{equation*}
Assuming Girth is greater than $\frac{4}{3}\log_{\Delta-1}\left( n \right)$  gives us a decoder against errors at weight at most $n^{\frac{3}{2}-\varepsilon}$ $\square$. 

\section{Decodeing and Testing}
For completeness, we show exactly how Theorem 1 implies testability. The following section repeats Leiverar's and Zemor's proof \cite{leverrier2022quantum}. Consider a binary string $x$ that is not a codeword. The main idea is the observation that the number of bits filliped by (any) decoder, while decoding $x$, bounds the distance $d\left( x,C \right)$ from above. In addition, the number of positive checks in the first iteration is exactly the number of violated restrictions.
%\begin{figure*}[h]
%\begin{adjustbox}{width=\textwidth}
\paragraph{Definition.}Let $L = \{L_{i}\}^{2|E|}_{0}$  be a series of $2|E|$. Such that for each vertex $ v \in V$ $\sum_{ e = \{u,v\} }{ L_{e_v} } \in C_{0}$. We will call $L$ a \textit{Potential list} and refer to the $e_{v}$'th element of $L$ as a suggestion made by the vertex $v \in V$ for the edge $e \in E$.Sometimes we will use the notation $L_{v}$ to denote all the $L$'s coordinates of the form $ L_{e_{v}} \forall e \in \text{Support} \left( v \right) $. Define the \textit{Force} of $L$ to be the following sum $  F\left( L \right) = \sum_{e = \{v,u\} \in E }{ \left(L_{e_v} + L_{e_u}\right) }$ and notice that $ F\left( L \right) \in C_{\oplus}$. And define the \textit{state} $S(L) \subset \mathbb{F}^{|E|}_{2}$ of $L$ as the vector obtained by choosing an arbitrary value from $ \{ L_{e_v}, L_{e_u} \}$ for each edge $e \in E$.  
\paragraph{Claim.} Let $L$ be the Potential list. If $F(L)=0$ then $S(L)\in C$.
\paragraph{Proof.} Denote by $\phi\left( e \right) \subset \{ L_{e_v}, L_{e_u} \}$ the value which was chosen to $e = \{v,u\} \in E$. By $F\left(L\right) = 0$ , it follows that $ L_{e_v} + L_{e_u} = 0 \Rightarrow L_{e_v} = L_{e_u} = \phi\left( e \right) $ for any $e \in E$. Hence for every $v\in V$ we have that $ S\left( L \right)|_{v} = \sum_{u \sim v}{ \phi\left( \{v,u\} \right) } =  \sum_{u \sim v}{ L_{e_v }} \in C_{0}$ $ \Rightarrow S\left( L \right) \in C \square$   

The decoding goes as follows. First, each vertex suggests the closet $C_{0}$'s codeword to his local view. Those suggestions define a Potential list, denote it by $L$, then if $F\left( L \right) <\tau$, by Theorem 1, one could find a suggestion of vertex $v$, and a codeword $c_v$ such that updating the value of $L_{v} \leftarrow L_{v} + c_{v}$ yields a Potential list with lower force.Therefore repeating the process till the force vanishes, obtain a Potential list in which its state is a codeword. 
\paragraph{Definition.} Let $\tau > 0, f : \mathbb{N} \rightarrow \mathbb{R^{+}}$, and consider a Tanner Code $C = \mathcal{T}\left( G, C_{0} \right)$. Let us Define the following decoder, and denote it by $\mathcal{D}$.  
\paragraph{}
\begin{algorithm}[H]
  \caption{Decoding}
  \label{alg:three}
  \KwData{ $x \in \mathbb{F}_{2}^{n}$ }
\KwResult{ $\arg\min {\left\{  y \in C : |y + x|  \right\} }$ if $d(y,C) < \tau $ and False otherwise. }
$ L \leftarrow \text{Array} \{ \} $\\
  \For { $ v \in V$} {
  $c^{\prime}_{v} \leftarrow \arg\min {\left\{  y \in C_{0} : |y + x|_{v} |  \right\} } $\\
  $ L_{v} \leftarrow c^{\prime}_{v}$
 }
 $ z \leftarrow \sum_{v \in V}{c^{\prime}_{v}} $\\
 \eIf{ $ |z| < \tau \frac{n}{f\left( n \right)} $}{
  \While{ $|z| > 0$ }{
   find $v$ and $c \in C_{0}$ such that $|z + c_{v}| < |z|$\\
   $z \leftarrow z + c_{v}$ \\
   $ L_{v} \leftarrow  L_{v} + c_{v}$
   }
  }{
   reject. 
 }
 \Return  $S(L) $
 
\end{algorithm}

\paragraph{Theorem.} Consider a Tanner Code $C = [n, n\rho, n\delta]$ and the disagreement code $C_{\oplus}$ defined by it. Suppose that for every codeword $ z \in C_{\oplus}$ in $C_{\oplus}$ such that $|z| < \tau^{\prime} n / f\left(n\right)$, there exists another codeword $y \in C_{\oplus} $ such that $|y| < |z|$, set $\tau \leftarrow \frac{\tau^{\prime} }{6 \Delta} \delta$ then, 

\begin{enumerate}
  \item $\mathcal{D}$ corrects any error at a weight less than $\tau n / f\left(n\right)$.   
  \item $C$ is $f\left( n \right)$ testable code.
\end{enumerate}


\paragraph{Proof.} So it is clear from the claim above that if the condition at line (6) is satisfied, then $\mathcal{D}$  will converge into some codeword in $C$. Hence, to complete the first section, it left to show that $\mathcal{D}$ returns the closest codeword. Denote by $e$ the error, and by simple counting arguments, we have that $\mathcal{D}$ flips at most:  
\begin{equation*}
  \begin{split}
    d_{\mathcal{D}}\left( x, C \right) & \le 2|e|\Delta + \tau \frac{n}{f\left( n \right)}\Delta
  \end{split}
\end{equation*}
bits. Hence, by the assumption, 
\begin{equation*}
  \begin{split}
    d_{\mathcal{D}}\left( x, C \right) & \le 3\Delta \tau \frac{n}{f\left( n \right)} \le 3\Delta \tau\delta n < \frac{1}{2} \delta n  
  \end{split}
\end{equation*}
Therefore the code word returned by $\mathcal{D}$ must be the closet. Otherwise, it contradicts the fact that the relative distance of the code is $\delta$.
To obtain the correctness of the second section, we will separate when the conditional at line (5) holds and not. And prove that the testability inequality holds in both cases. 
Let $x \in \mathbb{F}_{2}^{n}$ and consider the running of $\mathcal{D}$ over $x$. Assume the first case, in which the conditional at the line (5) is satisfied. In that case, $\mathcal{D}$ decods $x$ into it's closest codeword in $C$. Therefore:
\begin{equation*}
  \begin{split}
    d\left( x, C \right) \le & \ d_{D} \left( x, C \right) \le m\xi\left( x \right)\Delta +  |z|\Delta  \\ \le &  \  m\xi\left( x \right)\Delta + m\xi\left( x \right)  \Delta^{2} \\ 
     \frac{d\left( x, C \right)}{n} \le & \  \kappa_{1} \xi\left( x \right)    
  \end{split}
\end{equation*}
Now, consider the other case in which: $ |z| \ge \tau \frac{n}{f\left( n \right)}  $.
\begin{equation*}
  \begin{split}
    \frac{d\left( x, C \right)}{n} & \le 1 \le \frac{|z|}{\tau n}f\left( n \right) \le \frac{m}{n} \frac{1}{\tau} \Delta \xi\left( x\right)f\left( n \right) \\ & \le \kappa_{2} \xi\left( x \right)f\left( n \right)  
  \end{split}
\end{equation*}
Picking $ \kappa \leftarrow \max \{ \kappa_{1}, \kappa_{2} \}$ proves $f\left( n \right)$-testability $\square$
%\end{adjustbox}
%\end{figure*}
  \printbibliography 
\end{multicols*}
\end{document}


