\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[a4paper, total={7.5in, 10in} ]{geometry}
\usepackage{braket}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{svg}
\usepackage{float}
\usepackage{tikz}
\usetikzlibrary{patterns,shapes.arrows}
\usepackage{adjustbox}
\usepackage{tikz-network}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{multicol}
\usepackage[backend=biber,style=alphabetic,sorting=ynt]{biblatex}
\usepackage{xcolor}
\usepackage{pgfplots}
\DeclareUnicodeCharacter{2212}{−}
\usepgfplotslibrary{groupplots,dateplot}
\pgfplotsset{compat=newest}

\newcommand{\commentt}[1]{\textcolor{blue}{ \textbf{[COMMENT]} #1}}
\newcommand{\ctt}[1]{\commentt{#1}}
\newcommand{\prb}[1]{ \mathbf{Pr} \left[ {#1} \right]}
\newcommand{\expp}[1]{ \mathbf{E} \left[ {#1} \right]}
\newcommand{\onotation}[1]{\(\mathcal{O} \left( {#1}  \right) \)}
\newcommand{\ona}[1]{\onotation{#1}}
\newcommand{\PSI}{{\ket{\psi}}}
\newcommand{\LESn}{\ket{\psi_n}}
\newcommand{\LESa}{\ket{\phi_n}}
\newcommand{\LESs}{\frac{1}{\sqrt{n}}\sum_{i}{\ket{\left(0^{i}10^{n-i}\right)^{n}}}}
\newcommand{\Hn}{\mathcal{H}_{n}}
\newcommand{\Ep}{\frac{1}{\sqrt{2^n}}\sum^{2^n}_{x}{ \ket{xx}}}
\newcommand{\HON}{\ket{\psi_{\text{honest}}}}
\newcommand{\Lemma}{\paragraph{Lemma.}}
\newcommand{\PonB}{ \rho + \frac{5}{16}\delta\le \frac{3}{4} + \frac{1}{16} } 
\newcommand{\Cpa}{[n, \rho n, \delta n]}
%\setlength{\columnsep}{0.6cm}

\newcommand{\Gz}{ G_{z}^{\delta} } 
\newcommand{ \Tann } {  \mathcal{T}\left( G, C_0 \right) }
\newcommand{\xij} { X_{ij} } 
\newcommand{\indpr}{$ X_1, ..., X_n$ are independence random variables} 
\begin{document}

\title{Fourmlas Sheet.} 
\author{David Ponarovsky}
\maketitle
\begin{multicols*}{2}
  \subsection*{{\textcolor{orange}{Probability.}} } 
  \paragraph{Multiplicative Chernoff bound.} Suppose $ X_1, ..., X_n$ are independence random variables taking values in $\{0, 1\}$ Let $X$ denote their sum and let $\mu = \expp{\sum_{i}^{n}{X_{i}}} $  denote the sum's expected value. Then for any $\delta > 0$: 
%:<math>\Pr ( X > (1+\delta)\mu) < \left(\frac{e^\delta}{(1+\delta)^{1+\delta}}\right)^\mu.</math>
  \begin{equation*}
    \begin{split}
      & \prb{X \ge \left( 1+\delta \right) \mu }  \le e^{-2\frac{\delta^2\mu^{2}}{n}} \\ 
      & \prb{|X - \mu| \ge \delta\mu }  \le 2e^{-\delta^2\mu/3}, \qquad 0 \le \delta \le 1
    \end{split}
  \end{equation*}
  \paragraph{Bernstein inequalities.} \indpr with zero mean $\left( \mu = 0  \right)$. Suppose that $|X_{i}| \le M $ almost surely, for all $i$.Then, for all positive $t$:
  \begin{equation*}
    \begin{split}
      & \prb{ \sum^{n}_{i}{X_i} \ge t  } \le \exp\left( -\frac{\frac{1}{2}t^{2}}{\sum_{i}{\expp{X_{i}^{2}}} + \frac{1}{3}M}t \right)
    \end{split}
  \end{equation*} For example, consider coins taking values $\pm 1$ with probability $\frac{1}{2}$, then for every positive $\varepsilon$. 
  \begin{equation*}
    \begin{split}
      \prb{ | \frac{1}{n}\sum_{i}^{n}{X_i} | \ge \varepsilon  } \le 2\exp\left( -\frac{n\varepsilon^{2}}{2\left( 1+\frac{\varepsilon}{3} \right)} \right) 
    \end{split}
  \end{equation*}
  There is also a weakly dependent generalization version, that go as follow. Let $X_{0},X_{1},X_{2},\ldots X_{n}$ random variables. Suppose that for all integers $i$ it holds:  
  \begin{equation*}
    \begin{split}
      &\expp{X_{i}|X_{0},X_{1},X_{2},\ldots X_{i-1}}=0\\
      & \expp{X_{i}^{2}|X_{0},X_{1},X_{2},\ldots X_{i-1}}= R_{i}\expp{X_{i}^{2}}\\
      & \expp{X_{i}^{k} | X_{0},X_{1},X_{2},\ldots X_{i-1}}  \\
      & \ \ \le \frac{1}{2}\expp{X_{i}|X_{0},X_{1},X_{2},\ldots X_{i-1}} L^{k-2}k! 
    \end{split}
  \end{equation*}
  Then: 
  \begin{equation*}
    \begin{split}
      \prb{\sum_{i}^{n}{X_{i}} \ge 2t \sqrt{ \sum_{i=1}^{n}{R_{i}\expp{X_{i}^{2}}} } } \le \exp\left(-t^{2} \right)
    \end{split}
  \end{equation*}
  \paragraph{Jensen's inequality.} If $X$ is a random variable and $\phi$ is a convex function, then:

  \begin{equation*}
    \begin{split}
      \phi\left( \expp{X} \right) \le \ & \expp{\phi\left( X \right)} \Rightarrow   \expp{X} \le \phi^{-1} \left( \expp{\phi\left( X \right)} \right) \\
      & \expp{X} \le \ln\left(\expp{e^{X}}  \right)  \\ 
      &  \expp{X} \ge e^{\expp{\ln \left( X \right)}}  
    \end{split}
  \end{equation*}
  \paragraph{Paley–Zygmund inequality.} bounds the probability that a positive random variable is small, in terms of its first two moments. Could be thought as the lower bound Markov version. If a r.v $X$ is always positive and has a finate variance, then for $0 \le \tau \ge 1$:  
  \begin{equation*}
    \begin{split}
      & \prb{ X > \tau \expp{X} } \ge \left( 1 - \tau \right)^{2}\frac{\expp{X}^{2}}{\expp{X^2}}\\
      &  \prb{ X > \expp{X} - \tau \sigma } \ge \frac{\tau^{2}}{1+\tau^{2}} 
    \end{split}
  \end{equation*}
  \paragraph{Marcinkiewicz–Zygmund inequality.} \indpr with zero mean $\left( \mu = 0  \right)$ and $\expp{|X_{i}|^{p}} < \infty$, then there exist constants $A_{p}, B_{p}$ which depend only on $p$ such:  
  \begin{equation*}
    \begin{split}
      & A_{p}\expp{\left( \sum_{i}^{n}{|X_{i}|^{2}} \right)^{p/2}  } \le \expp{|\sum_{i}^{n}{X_{i}}|^{p} }\le B_{p}\expp{\left( \sum_{i}^{n}{|X_{i}|^{2}} \right)^{p/2}  }        
    \end{split}
  \end{equation*}
  \paragraph{Cauchy–Schwarz Expectation Inequality.} Let $X,Y$ be random variables then the inequality becomes: 
  \begin{equation*}
    \begin{split}
      |\expp{ XY} |^{2}\le\expp{X^{2}}\expp{Y^{2}}
    \end{split}
  \end{equation*}
  \paragraph{Union Of Pairwise Independent.}  Denote by $\{{A}_i, i \in \{1,2,...,n\}\}$ a set of  $n$ bernoulli events with probability of occurrence $\mathbb{P}(A_{i})=p_i$  for each $i$. Suppose the Joint probability distribution probabilities are given by $\mathbb{P}(A_{i} \cap A_{j})=p_{ij}$ for every pair of indices $(i,j)$. Kounias Bounds for the probability of a union by:
  \begin{equation*}
    \begin{split}
        \mathbb{P}(\displaystyle {\cup}_iA_{i}) \le  \sum_{i=1}^n p_{i}-\underset {j\in \{1,2,..,n\}}{\max} \sum_{i\neq j} p_{ij}, 
    \end{split}
  \end{equation*}
  which subtracts the maximum weight of a star spanning tree on a complete graph. Hunter-Worsley prove that is sufficient to consider only the wight of the minium spanning tree.
  \subsection*{{\textcolor{orange}{Inequalitys.}} } 
  \paragraph{Sedrakyan's inequality.} For any reals$a_{0},a_{1},a_{2},\ldots a_{n}$ and positive eals $b_{0},b_{1},b_{2},\ldots b_{n}$ we have:  
  \begin{equation*}
    \begin{split}
      \frac{a_{1}^2}{b_{1}}+\frac{a_{2}^2}{b_{2}}+ ...+ \frac{a_{n}^2}{b_{n}} \ge \frac{\left( a_{1}+ a_{2} + ... + a_{n}  \right)^{2}}{b_{1}+b_{2}+... b_{n}}  
    \end{split}
  \end{equation*}
  \subsection*{{\textcolor{orange}{Expanders.}} } 
  \paragraph{Second Eigenvalue.} Let $A$ be the adjacency matrix of $\Delta$ regular graph, then the seconed eignenvalue is:
  \begin{equation*}
    \begin{split}
      \lambda &= \max_{f \perp \mathbf{1}} { \frac{f^{\top}A f  }{ f^{\top}f}}
    \end{split}
  \end{equation*}
  An exapmle for usecase, consider the \emph{Cayley} Graph defined by the union of two generatorset and a homriphisem of it, namly $S$ and $gS$ for some $g \in $ the group. Then we have that the new spacrtial gap is at most two times the orignal one:    
  \begin{equation*}
    \begin{split}	
      \lambda^{\prime} &= \max_{f \perp \mathbf{1}} { \frac{f^{\top} \left( A_{S} + A_{gS} \right) f  }{ f^{\top}f}} \\
      & \le  \max_{f \perp \mathbf{1}} { \frac{f^{\top}A_{S} f  }{ f^{\top}f}} +  \max_{f \perp \mathbf{1}} { \frac{f^{\top}A_{gS} f  }{ f^{\top}f}} \\
      & \le \lambda + \lambda = 2\lambda
    \end{split}
  \end{equation*}
\end{multicols*}{2}
\end{document} 


