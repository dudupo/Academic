

%\newcommand*{\ACM}{}%

\ifdefined\ACM

%\documentclass[sigplan,screen]{acmart}
\documentclass[manuscript,screen,review]{acmart}

\else
 \documentclass[14pt]{article}
%\usepackage{libertine}
\usepackage{cuted}%\usepackage{widetext}
\input{./usepackage}
\usepackage{cancel}
\usepackage{subcaption}
\addbibresource{./sample.bib}

\fi

\begin{document}
\input{newcommands}
%\title{ $\textbf{QNC}_{1} \subset $ noisy-\textbf{BQP}}
\title{ Memory. }
%\author{Michael Ben-Or \ \ David Ponarovsky}
\maketitle

\newcommand*{\Mbas}{\mathcal{X}^\prime}
\newcommand*{\bas}{\mathcal{X}}
\newcommand*{\sMbas}{\Mbas}
\newcommand*{\QQ}{C_{X}/C_{Z}^\perp }
\newcommand*{\trig}{ Triorthogonal }
\newcommand*{\Hyp}{ Hyperproduct }
\newcommand*{\Cin}{ C_{\text{initial}} }
\newcommand*{\Ctan}{ C_{\text{Tan}} }



\newcommand*{\QACze}{\mathbf{QAC}_{0}}
\newcommand*{\QNCzef}{\mathbf{QNC}_{0,f}}
\newcommand*{\QNCze}{\mathbf{QNC}_{0}}
\newcommand*{\QNCon}{\mathbf{QNC}_{1}}
\newcommand*{\NCon}{\mathbf{NC}_{1}}
\newcommand*{\noiseQNCon}{noisy-$\QNCon$}
\newcommand*{\QNC}{\mathbf{QNC}}
\newcommand*{\QNCG}{\mathbf{QNC_G}}
\newcommand*{\NC}{\mathbf{NC}}
\newcommand*{\QNCiG}{\mathbf{QNC_{G,i}}}


% Constant depth fault tolerance construction.   
\newcommand*{\CDO} {CDFT} 

\section{Relaxation to The Fault Tolerance Model.} We are interested in the following extension to the fault-tolerant circuit model. We are equipped with an additional type; in each turn, a strong entity, which we trust, sets a hint $I_{t}$ on the type. We would like to minimize $|I| := \min_{t} |I_{t}|$. In particular, a fault-tolerant construction in the standard model exhibits a fault-tolerant construction in the relaxed model with $|I| = 0$.

Another example is using the hints given by the strong entity for either deciding what correction should be applied or what 'gate-teleportation correction' should be applied. It is easy to check that previous constructions give relaxed fault tolerance such that:     
\begin{enumerate}
  \item They output encoded states with non-trivial distance. 
  \item They exhibit only a constant overhead in depth. 
  \item At each turn, $|I_{t}| / \text{ logical qubits }$ depends on the code length.  
\end{enumerate}
That brings us to the following question:
\begin{oproblem}
Is there a relaxed fault tolerance scheme that benefits from the first and second bullets above, yet requires a hint at a length that is constant per logical qubit? Namely:
  \begin{equation*}
    \begin{split}
      \frac{ |I| } { \text{ logical qubits } } = O(1) ? 
    \end{split}
  \end{equation*}
\end{oproblem}

\newcommand{\TT}{ \mathcal{T} }

\section{Notations and Definitions.} 
Consider a code with a left $k$-colorized Tanner graph $\TT{}$, such that any two left bits of the same color share no check. For a subset of bits $S$, we denote by $S_{c_1}$ its restriction to color $c_1$. We use the integer $\Delta$ to denote the right degree of $\TT{}$. Our computation is subject to $p$-depolarized noise. We denote by $m$ the block length of the code. The decoder works as follows:
\begin{enumerate}
  \item On the hint-type Pick a random color. 

    \ctt{In the relaxed version: the 'right/best' color is given by the strong entity.} 
  \item For any (q)bit at that color, check if flipping it decreases the syndrome. If so, then flip it.
\end{enumerate}


%We say that a density matrix $\rho$, induced on the $m$-length block, is a \textbf{good noisy distribution} if:
%\begin{enumerate}
  %\item $\rho$ is subjected to $q$ - local stochastic noise. 
  %\item Denote by $S$ the support of an error occurring on $\rho$ (S is a random variable). Then, with high probability\footnote{I'm leaving specifying what it is to later.}, $|S_{c_{1}}| > \frac{1}{4} |S|$. 
%\end{enumerate}


\begin{claim}
Let $\TT$ be a Tanner graph such that $\Delta > 2k$. There is $p_{0} \in (0,1)$ and $q \in (0,1)$ such that for any $p < p_{0}$ and a density $\rho$, which is subjected to $q$-local stochastic noise, there is a color $c_{1}$ such that after a cycle of absorbing $p$-depolarized noise and correcting according to the decoding rule when color $= c_{1}$, the resulting state $\rho^\prime$ will remain subjected to $q$-local stochastic noise.
\end{claim}
\begin{figure}[h]
  \begin{center}
  \tikz{ 
    \node  (A) at (1,0) { Given - $\rho$ };
    \node  (B) at (5,0) { Decoding };
    \node  (C) at (9,0) { $p$-depololraized };
    \draw[->] (A) to (B); 
    \draw[->] (B) to (C); 

    \node  (E) at (1,-1) { $E_{1}$ };
    \node  (D) at (5,-1) { $E_{2}$ };
    \node  (F) at (9,-1) { $E_{3}$ };

    \draw[->] (A) to (E); 
    \draw[->] (B) to (D); 
    \draw[->] (C) to (F); 
  }
\end{center}
\caption{Illustration of the cycle.}
\end{figure}
\subsection{Proof.}
First, let's bound the probability that the error after the decoding round ($E_{2}$) is supported on $S$. (We use here the fact that views of the bits through their stabilizer don't overlap since we took only bits of the same color for the decoding.)
\begin{equation*}
  \begin{split}
    \prb{ \Su{} \left(E_{2}\right) = S } \le \prb{\text{any bit } v\in S_{c_{1}} \text{ sees majority of satisfied checks   } } \le q^{\frac{1}{2}\Delta |S|_{c_{1}}}
  \end{split}
\end{equation*}
Now, to roughly analyze the error after observing a round of $p$-depolarized noise, we consider a model in which new errors due to the depolarized channel don't correct previous errors. Thus, we get:
\begin{equation*}
  \begin{split}
    \prb{ \Su{} \left(E_{3}\right) = S   } & \le  \sum_{S^{\prime} \subset S }{q^{\frac{1}{2}\Delta|S^{\prime}|_{c_{1}}}p^{|S /S^{\prime}|}  }  %+  \prb{  | \Su{} \left( E_{2} \right)_{c_1} | < \frac{1}{4} | \Su{} \left( E_{2} \right) |} \\
    %&\le \left( q^{\frac{1}{4}\Delta} + p \right)^{|S|}  + \prb{  | \Su{} \left( E_{2} \right)_{c_1} | < \frac{1}{4} | \Su{} \left( E_{2} \right) |} 
  \end{split}
\end{equation*}

On the other hand, 
\begin{equation*}
  \begin{split}
    \sum_{c_{i}}|S|_{c_{i}} &= k\cdot \expp{|S|_{c_{i}}} = |S| \\
    & \Rightarrow \max_{c_i}|S|_{c_{i}} \ge \frac{1}{k}|S|
  \end{split}
\end{equation*}
So if $c_{1}$ is the color that maximizes $|S|_{c_1}$, then:

\begin{equation*}
  \begin{split}
    \prb{ \Su{} \left(E_{3}\right) = S   } &  \le \sum_{S^{\prime} \subset S }{q^{\frac{1}{2}\Delta|S^{\prime}| / k }p^{|S /S^{\prime}|}  } \\
    &\le \left( q^{\frac{1}{2k}\Delta} + p \right)^{|S|} \le q^{|S|} 
  \end{split}
\end{equation*}
\newcommand*{\Pt}{\mathcal{P}}


\section{Suitable Codes.}  
We first show that the partition code has a representation (a check matrix) for which the induced $\TT$ satisfies the relation $\Delta > 4k$, and then show that the hypergraph product code defined by multiplying the Tanner graphs of that representation gives $\Delta > 2k$.

\begin{claim}
Let $C$ be a code with a Tanner graph $\TT$. Denote by $\TT^\top$ the Tanner graph of the transpose code and by $Q\left(\TT \times \TT^\top\right)$ the Tanner graph obtained by the hypergraph product. Then:
  \begin{enumerate}
    \item $ \Delta\left(Q\left(\TT \times \TT^\top\right)\right) = \max \{  \Delta(\TT), \Delta(\TT^{\top}) \} $ 
    \item $ k\left(Q\left(\TT \times \TT^\top\right)\right) \le  k(\TT) + k(\TT^{\top})$ 
  \end{enumerate}
\end{claim}

\begin{proof} Easy.
\end{proof}

\begin{claim}
The repetition code has a representation for which $\Delta > 4k$.
\end{claim}
\begin{proof}
Denote by $H_{0}$ the checks obtained by treating the repetition code as a Tanner code over the cyclic graph. Observe that $k_{0} = 2$ and $\Delta_{0} = 2$.

Now, let $V^{+}, V^{-}$ be a partition of the bits according to their color. Any check of the form $v^{+} + v^{-}$ where $v^{\pm} \in V^{\pm}$ agrees with the coloring. So, by adding a perfect matching, we increase $\Delta$ by $1$ and keep the colorization. We have $\sim n/2!$ such matchings, so we can add $100\Delta$ and get the correction of the claim.

Furthermore, the length of the transposed code increases by the number of checks we add, and its distance can't decrease. So, we get that the parameters of the transposed code are $[n + 100\Delta n, 1, \ge n]$.

\end{proof}

Hence, we have a simple code that can serve as memory in the relaxed setting. Yet, it doesn't provide a solution to the problem since the dimension of the code is non-trivial\footnote{Can be decreased to $ \Theta( \sqrt{n}) $ if we choose $C_1 = C$ and $C_{2}$ to be the transposed code instead of choosing $C_{1} = C_{2} = C$.}:
\begin{equation*}
  \begin{split}
    K_{Q} = K_{1}K_{2} + K_{1}^\top K_{2}^{\top} \ge O(1) + \Theta(n)
  \end{split}
\end{equation*}
Thus, we will still need to perform a non-trivial computation for the gate-teleportation gadget.

\ctt{ In fact, since $\dim \TT{}^\top \ge |C| - |V| $ and $|V| \Delta = |C| \Delta_{2} \le |C| k  \le |C| \frac{1}{2} \Delta $ we get that $\dim \TT{}^\top \ge \Theta ( n ) $. For the hypergraph product. } . 



\begin{equation*}
  \begin{split}
    \frac{1}{2}\Delta  \le (1-\varepsilon)\Delta \le  \frac{\Delta}{k} \le \frac{\Delta}{\Delta_{2}} \le \frac{|\Gamma (A)| }{|A|} 
  \end{split}
\end{equation*}



%So, it remains to show that property (2) still holds with high probability. The following is incorrect, yet almost correct. I want to say that a new error observed by the depolarized channel has to spread evenly on bits at color $c_{1}$, and by concentration get that they are far away from $\frac{1}{4}$ with probability less than $\text{exp}( - \varepsilon m )$.
%
%Then, let $S^{t} = \Su \left(E\right) $ at time $t$ and denote by $\Pt_{t}$ the probability that $|S^{t}_{c_{1}}| > \frac{1}{4}|S^{t}|$. Then:
%\begin{equation*}
  %\begin{split}
    %\Pt_{t+1} &\ge \prb{ |S^{t}_{c_{1}}| > \frac{1}{4}|S_{t}| \text{ and }  | \left( S_{t+1}/S_{t} \right)_{c_1}  | \ge \frac{1}{4}|S_{t+1}/S_{t}|  }  \\
    %&\ge \Pt_{t} \cdot \left( 1 - e^{- \varepsilon m}   \right) \ge \Pt_{0}\left( 1 - e^{- \varepsilon m } \right)^{t+1} \\ 
    %& \ge \Pt_{0}\left( 1 - (t+1) e^{- \varepsilon m } \right)
  %\end{split}
%\end{equation*}
%
%There is a problem with the assumption that the new error spreads uniformly across the colors. In particular, $m$ should be taken as the untapped qubits, so it changes over time and might not contain qubits of color $c_{1}$ at all.
%
%
  %%\section{ Strategies to get \CDO. }  \label{sec:opt}
 %%
%%The second gadget is Memory, a particular type of code which allows restraining the error rate by exhibiting a constant depth procedure that, when promising that the error rate is below a threshold, suppresses the error by at least a constant factor. Using memory, we will be able to promise with high probability that the error rate is lower than some fraction. 
%%
 %%\subsection{Memory.} 
 %%\newcommand*{\DD}{\mathbf{D} }
%%Informal memory code is a code that stores a logical state for a long time while keeping the noise below a certain amount. We define it formally by saying that memory codes will reduce an error that affects at most $\beta$ portion of the qubits into an error that affects at most $\gamma$ portion of the qubits.
%%
 %%\begin{definition}[Ideal $(\beta,\gamma)$-Memory]
   %%We say that a (quantum) error correction code $C$ is an Ideal $(\beta,\gamma)$-Memory code if there is a constant depth procedure $\DD$ such that for any $I$ of size $|I| \ge (1 - \beta) n$ and a mixed states $\sigma$ and $\rho$ such $\sigma$ distributed over the $C$'s codewords $\sigma \in C$ and $\Tr_{I}\left(\rho\right) = \Tr_{I}\left(\sigma\right)$, we have that there is subset of qubits $J$ at size at least $(1-\gamma)n$:
   %%\begin{equation*}
     %%\begin{split}
        %%\Tr_{J} \DD \left(\rho\right) = \Tr_{J}\left(\sigma\right) 
     %%\end{split}
   %%\end{equation*}
%%
 %%\end{definition}
%%We would like to extend the memory gadgets to work with high probability, which motivates us to define the following:
%%\newcommand*{\Po}{\mathcal{P}_{1}}
%%\newcommand*{\Pt}{\mathcal{P}_{2}}
%%\newcommand*{\Nn}{\mathcal{N}}
%%\begin{definition}[ $\left(\Po,\Pt \right)$- thermal couple. ]
%%Let $\Po,\Pt$ be sets of density matrices induced over the $n$-qubit Hilbert space, and let $\Nn$ be a $p$-stochastic local noise channel for some constant $p \in (0,1)$. We say that the couple $\left(\Po,\Pt \right)$ is a thermal couple if for any $\rho \in \Pt$, we have $\Nn(\rho) \in \Po$ with high probability.
%%\end{definition}
%%
 %%\begin{definition}[$(\Po,\Pt)$-Memory]
   %%Consider a $\left(\Po,\Pt \right)$- thermal couple, We say that C is a $(\Po, \Pt)$-Memory if there is a constant depth procedure $\DD$, such that for any $\rho \in \Po$ we have $\DD\left( \rho \right) \in \Pt$, with high probability. 
 %%\end{definition}
 %%For example, consider a code $C$ with a $\Delta$-regular Tanner graph. Let $\Po$ be all the noisy states derived from codewords in $C$ such that the syndrome graph induced by them can be decomposed into disjoint $\Delta/2$-connected components $A_{1},A_{2},..A_{l}$, each of size at most $|A_{i}| < \beta \sqrt{n}$, and the $\Delta/2$-distance between any two of them $A_{i}, A_{j}$, namely the number of edges needed to add to merge them into one single $\Delta/2$-connected component, is at least $\theta \min \left( |A_{i}|, |A_{j}| \right)$. We call such decomposition characterization $(\beta \sqrt{n}, \theta )$ error decomposition. 
%%
 %%Now let $\Pt$ be all the deviations from $C$, such that the syndrome graph induced by them can be decomposed into $(\gamma \sqrt{n}, \frac{\beta}{\gamma}  \theta )$ error decomposition. The couple $\left( \Po, \Pt \right)$ is thermal couple, And combining the quantum expander code and the parallel small set-flip decoder \cite{grospellier:tel-03364419} they defines a $\left( \Po, \Pt \right)$-memory. 
%%
%%
 %%\newcommand{\Px}[2]{P^{(v)}_{#1}(#2)}
%%
%%\begin{claim}
  %%The probability to have $\Px{\alpha\Delta}{x} \le $ 
%%\end{claim}
%%
%%\begin{claim}
  %%Any $\alpha\Delta$-connected component $E$ can be decompized to $\alpha\Delta-1$ connected component and more $\Theta( E/\Delta^{3})$ edges. 
%%\end{claim}
%%
%%\begin{proof}
  %%$E$ is connected. Let $T$ be its spanning tree. Now consider $Y$, a subset of edges obtained by colorizing from any vertex at an odd level of $T$ a single forward edge. And let $E^{\prime} = E/Y$. First, observes that $E$ is an $\alpha\Delta - 1$ connecnted componnent. On the otherhand: 
  %%\begin{equation*}
    %%\begin{split}
      %%|Y| &= \frac{1}{\Delta-1}\sum_{i}^{h/2} E\left( T^{2i + 1} \right) =  \frac{1}{\Delta-1}\sum_{i}^{h/2} \frac{1}{2} \left( E\left( T^{2i + 1} \right)+ E\left( T^{2i + 1} \right) \right) \\  
      %%& \ge \frac{1}{\Delta-1}\sum_{i}^{h/2} \frac{1}{2} \frac{1}{\Delta} \left( E\left( T^{2i + 1} \right)+ E\left( T^{2i} \right) \right) = \frac{1}{2 \left( \Delta - 1 \right) \Delta }|T| \\ 
      %%& \ge \frac{1}{2 \left( \Delta - 1 \right) \Delta } \frac{1}{\Delta}|E| \ge \frac{1}{2\Delta^{3}}|E|  \\
      %%& \left( \ge \frac{1}{2 (\Delta - 1) \Delta } \left( V(T) - 1 \right)     \right) 
    %%\end{split}
  %%\end{equation*}
%%\end{proof}
%%
%%
%%\begin{proof}
  %%Assume that $J$ is vertices subset that support an $\alpha\Delta$ connected $E$ in $G$, then it's also the support of $\alpha\Delta -1$ connected, denote by $E^{\prime}$ that sub componnent. So we can construct $E$ by first sample $E^{\prime}$ and then find a mathcing between the left veritcis. Thus:  
  %%\begin{equation*}
    %%\begin{split}
      %%\Px{\alpha\Delta}{x} \le \Px{\alpha\Delta - 1}{x} \cdot (p)^{\frac{x}{2\Delta^{2}}} \le ( p)^{\frac{x}{2 \Delta^{2} } \alpha \Delta } =  ( p)^{\frac{\alpha \Delta }{2 } x}
    %%\end{split}
  %%\end{equation*}
%%\end{proof}
%%
%%\begin{claim}
  %%The ptobability to have $n^{\varepsilon}$ connencted component is: 
%%\end{claim}
%%\begin{proof}
  %%\begin{equation*}
    %%\begin{split}
      %%\le &  n \sum_{n^{\varepsilon}}^{n} \sum_{v \in V}\Px{\alpha\Delta}{x}   
      %%\le     n  \frac{  (\Delta p)^{\frac{n^{\varepsilon}}{2} \alpha \Delta }}{ 1  - (\Delta p)^{ \frac{1}{2}\alpha \Delta }} \rightarrow 0 
    %%\end{split}
  %%\end{equation*}
%%\end{proof}
%%
%%
%%
%%
%%
%%
%%
%%
  %%\newcommand{\sliceb}[1]{ \slice[style=blue, label style={inner sep=1pt,anchor=south west,rotate=40}]{#1}}
%%
%%%\slice{ $\gamma\left( c \alpha + p  \right) + p < \alpha $ error rate. }
%%%\slice{ $c\alpha + p$ error rate.  } 
  %%%row sep=0.3cm, column sep=0.7cm,
%%%slice style=blue,slice label style={inner sep=1pt,anchor=south west,rotate=40}
  %%% \slice{ $\rho$ at $\alpha$ error rate. }
  %%\begin{figure}[h]
    %%\centering
    %%\begin{quantikz}[slice style=blue,slice label style={inner sep=1pt,anchor=south west,rotate=40}]
      %%\lstick{$q_1$} & \qw & \qw & \qw &  \sliceb{  $ \rho$ at $\alpha$ error rate.  }  & \gate[wires=9][1.7cm]{U}  \sliceb{ $c\alpha + p$ error rate.  }  & \gate[wires=9][1.7cm]{ F }  & \gate{\mathcal{N}}  \sliceb{ $\gamma\left( c \alpha + p  \right) + p < \alpha $ error rate. }& \qw & \\
  %%\lstick{$q_2$} & \qw & \qw & \qw &  &                  &    & \gate{\mathcal{N}} & \qw & \\
  %%\lstick{$q_3$} & \qw & \qw & \qw &  &                  &    & \gate{\mathcal{N}} & \qw & \\
  %%\lstick{$q_4$} & \qw & \qw & \qw &  &                  &    & \gate{\mathcal{N}} & \qw & \\
  %%\lstick{$q_5$} & \qw & \qw & \qw &  &                  &    & \gate{\mathcal{N}} & \qw & \\
  %%\lstick{$q_6$} & \qw & \qw & \qw &  &                  &    & \gate{\mathcal{N}} & \qw & \\
  %%\lstick{$q_7$} & \qw & \qw & \qw &  &                  &    & \gate{\mathcal{N}} & \qw & \\
  %%\lstick{$q_8$} & \qw & \qw & \qw &  &                  &    & \gate{\mathcal{N}} & \qw & \\
  %%\lstick{$q_9$} & \qw & \qw & \qw &  &                  &    & \gate{\mathcal{N}} & \qw & 
%%\end{quantikz}
%%\caption{ Usage of  Ideal $(\beta,\gamma)$-Memory to obtain fault tolerance computation. }
    %%\label{fig:mem}
  %%\end{figure}
%
%( \ctt{ See the comment in blue below, it gets complicated.} )
%
%
%
\textbf{Question.}  
Consider the $n$-dimensional toric code, where qubits are placed on $k$-cells of the $n$-dimensional hypercubic lattice.  
For an $i$-cell, denote by $\Delta_i^{+}$ the number of $(i{+}1)$-cells adjacent to it, and by $\Delta_i^{-}$ the number of $(i{-}1)$-cells adjacent to it.  
For which values of $k$ do both of the following strict inequalities hold?
\[
\Delta_k^{+} > \Delta_{k+1}^{-}, \qquad \Delta_k^{-} > \Delta_{k-1}^{+}.
\]

\textbf{Answer.}  
In an $n$-dimensional hypercubic lattice one has
\[
\Delta_i^{+} = 2\,(n-i), \qquad \Delta_i^{-} = 2\,i.
\]
Therefore, the two inequalities become
\[
2(n-k) > 2(k+1) \quad\iff\quad k < \tfrac{n-1}{2},
\]
\[
2k > 2\bigl(n-(k-1)\bigr) \quad\iff\quad k > \tfrac{n+1}{2}.
\]
These conditions are mutually exclusive, since they require simultaneously
\[
k < \frac{n-1}{2} \quad \text{and} \quad k > \frac{n+1}{2}.
\]
Thus, there is no value of $k$ (for any dimension $n$) for which both inequalities hold at once.  

%In particular, for the $10$-dimensional toric code, condition (A) requires $k<4.5$ while condition (B) requires $k>5.5$, which is impossible. Hence, no such $k$ exists

Yet, if one is willing to satisfy only the first inequality. Then: 
\begin{equation*}
  \begin{split}
    1 < \frac{\Delta_{k}^{-}}{\Delta_{k-1}^{+}} = \frac{2k}{2(n-(k-1))} \rightarrow k > \frac{2}{3}n
  \end{split}
\end{equation*}

\textbf{Should be verified:}
\begin{enumerate}
  \item In addition the dimension of the code should be ${ n \choose k } $. (Also known as the Betti numbers). 
  \item Numebr of $k$-cells shared by a $j$ - cell and a $i$ -cell. $  { j - i \choose  k- i }  $. 
  \item The partiy of ${ 2l \choose l }  $.  
  \item should understand: \href{https://math.stackexchange.com/questions/3242404/homology-of-n-dimensional-torus}{Math stachexhange}.
\end{enumerate}



\section{Amplification.} 


\begin{claim}
  Consider the tanner graph of the classic code $C_X$ which can be used to construct a quantum LDPC code. For any constant $c \in (0,1)$ there exist $\gamma < c$  and a subset of qubits $B$ at size  $B = \gamma n < c n$ such that $|\Gamma(B)| < \frac{1}{2} \Delta B$. 
\end{claim}
\begin{proof}
  Easy. 
\end{proof}

Now consider the following construction, we pick $c = \frac{1}{\Delta^{2}}$, and $B$ at size lower than $\gamma n$, denote by $A$ the complement of $B$. Now pick $\Theta$ hash functions $ \Theta = \{ h : \Gamma(A) \rightarrow \Gamma (B) / \Gamma (A) \}$. 
One one hand: 


\begin{equation*}
  \begin{split}
    \frac{|\Gamma(A)|}{ |\Gamma(B)/\Gamma(A)| } & \ge  \frac{|\Gamma(A)|}{ |\Gamma(B)| } \ge    \frac{|\Gamma(A)|}{ \frac{1}{2}\Delta|(B)|}\\
  & \ge \frac{\left( 1 - \gamma \right) n}{  \frac{1}{2} \gamma n  } \ge 2\Delta ( 1 - \frac{1}{\Delta} ) \ge \Delta
  \end{split}
\end{equation*}

In addition, $| \Gamma(B) / \Gamma(A) | \cdot \Delta_{2} \ge |B| \Rightarrow | \Gamma(B) / \Gamma(A) |  \ge |B|/\Delta_{2}$ and hence:

\begin{equation*}
  \begin{split}
    \frac{|\Gamma(A)|}{ |\Gamma(B)/\Gamma(A)| } & \le     \frac{|\Gamma(A)|\Delta_{2}}{ |B| } \le \frac{|A|}{|B|} \Delta \Delta_{2}
  \end{split}
\end{equation*}

Notice that if for any $B$ at the range $(1/\Delta^{3}, 1/\Delta^{2}) n$ we have that $|\Gamma(B)| \ge \frac{1}{2} \Delta |B|$ then it means that there is no codeword in $C_{X}$ at weight  $(1/\Delta^{3}, 1/\Delta^{2}) n$, thus we if use the $(n,k)$-Toric we can also finds $B$ at size $\ge 1/\Delta^{3} n $. 

We add the following checks $ X_{i} \cdot X_{\theta(i)} $. The degree of the bits changes as follow:   
\begin{enumerate}
  \item If $v \in A$ then: $\Delta(v) = \Delta + |\Theta|\Delta $
  \item Else, namely $v \in B$, denote by $\Delta_{l}, \Delta_{r}$ the degree of $v$ when restricted to $\Gamma(A)/\Gamma(B)$ and  $\Gamma(B)/\Gamma(A)$,and define $\xi = \Delta_{l} |\Theta|    + \Delta_{r}|\Theta| \cdot \left(  \frac{|\Gamma(A)|}{ |\Gamma(B)/\Gamma(A)| } \right)^{-1} $ then: $ \Delta + \xi   \ge \Delta(v) \ge \Delta + \frac{1}{2}\xi$.  
\end{enumerate}

  
\section{Classical Case, Repetition Code.}
In each decoding iteration, we pick random triples and set the values of their bits to the majority. Now, assume that the probability of a subset $S$ being an error is less than $q^{|S|}$, and denote by $X_{i}$ the random variable that counts the number of triples for which $i$ of their bits belong to $S$. On one hand:
\begin{equation*}
  \begin{split}
    |S| &= \sum_{i} i X_{i} = X_{1} + 2X_{2} + 3X_{3} 
  \end{split}
\end{equation*}
On the other, after a decoding cycle the probability of an error is: 
\begin{equation*}
  \begin{split}
    \prb{ E_{2} = S  } &= q^{2X_{1} + 2X_{2} + 2X_{3}} = q^{|S| + X_{1} - X_{3}}  
  \end{split}
\end{equation*}
Now for $|S| \le n/3$ we have that $ \expp{X_{1}- X_{3}} \ge |S|/4 $. \ctt{Require proof.}

So, we find that after a cycle of correction and $p$-depolarized noise accumulation, we have:
\begin{equation*}
  \begin{split}
    \prb{E_{3} = S} \le \left( q^{\frac{5}{4}} + p \right)^{|S|} \le q^{|S|}
  \end{split}
\end{equation*}

And if the distance of any small check is $d$, then we would find that the exponent of $q$ is:
\begin{equation*}
  \begin{split}
    |S| + \sum^{d}_{i=1}\left( d-i \right)X_i - \sum_{i=d+1}^{\Delta} \left( i - d \right)X_{i}
  \end{split}
\end{equation*}

For expander checking\footnote{Two rounds of fixed checks are taken such that the parity check matrix is an expander according to some expansion measure.}, we have that having an error over $|S|$ qubits after a correction round implies that the error spread over $\left( 1 + \varepsilon \right)|S|$\footnote{Conditioned on the assumption that in the previous moment for the decoding there were fewer than $\gamma n$ faulty bits.}. Thus, we get that with high probability:

\begin{equation*}
  \begin{split}
    \le \left( q^{1+\varepsilon} / \left( 1 - o(f) \right)   + p \right)^{|S|}
  \end{split}
\end{equation*}


\section{Another Decoder.}

Consider the follow decoder: Any bit picks two random checks adjoin to it. If they both unsatisfied it flips itself. The idea, is that we might control the amount of independence. 

\newcommand*{\Xuv}{X_{u}^{v}}

Let $X_{u}^{v}$ be the indicator, indicating that the bits $v$ and $u$ choose checks that share a bit, we call it a collision, To compute he expected number of collisions, we define the graph given by $4$-steps walk over the Tanner graph. Then:  

\begin{equation*}
  \begin{split}
    \sum \expp{X_{u}^{v}} = \frac{1}{\Delta^{2}} \cdot 1_{S}^{\top} A_{G} 1_{S} \le \frac{1}{\Delta^{2}} \frac{ \left( \Delta\Delta_{2} \right)^{2} |S|^{2} }{n} + \frac{1}{\Delta^{2}} \lambda \left( \Delta \Delta_{2} \right)^{2} |S|
  \end{split}
\end{equation*}
Where $S$ is the subsets of the faulty bits. So if the $4$-steps graph is Ramanujan, namely $\lambda \approx \Delta \Delta_{2}$: 
\begin{equation*}
  \begin{split}
    \approx \frac{\Delta_{2}^{2}|S| }{n} + \frac{\Delta_{2}}{\Delta} |S| \rightarrow  \frac{\Delta_{2}}{\Delta} |S| 
  \end{split}
\end{equation*}

It's even easier to bound the expectation of the pairs $u$ and $v$ which share the same check. So given a checks choice, we have that the probability of error to kept: 
\begin{equation*}
  \begin{split}
  \sim q^{ \left( 2 - \frac{\Delta_{2}}{\Delta} \right) |S| }
  \end{split}
\end{equation*}
Conditioned on the case that the number of collision is not far from the expectation.  


\section{Concentration in Local Stochastic Noise.} 

Denote by $\mu$ the expected flips, and by $\alpha,\beta < 1$ marginal parameters such the volume of picking $(\mu - \beta, \mu + \beta)$ is larger than picking $( \ge \mu + \alpha)$. 
\begin{equation*}
  \begin{split}
    \prb{ \frac{|S|}{n} \ge \mu + \alpha }  & \le \sum_{m \ge \mu + \alpha } Cq^{n(\mu + \alpha)} \le q^{n(\alpha - \beta) }  \sum_{m \ge \mu + \alpha } \star \\ 
      & \le  q^{n(\alpha - \beta) } \prb{  |\frac{|S|}{n} - \mu | \le \beta   } 
  \end{split}
\end{equation*}

Other way, first use the Markov inequality to bound the space that $ \ge n\alpha $ so we get $\ge \frac{1}{2} \cdot \frac{1}{\alpha} \cdot 2^{n}$, and then use the local stochastic property:   
\begin{equation*}
  \begin{split}
    \le C \frac{ \frac{1}{2} }{ \alpha } \cdot 2^{n} \cdot q^{\alpha n} = \frac{\alpha}{2} C \left( 2 q^{\alpha} \right)^{n}
  \end{split}
\end{equation*}
For big enough $\alpha$ we see that the probability decay exponentially. (An also way to prove that $q^{q} \ge \frac{1}{2}$). 

\section{Bounding $E(S,S)$, fixed $|S|$, by Branuli Process. } 

First, consider $\mathcal{D}$ that pick any vertex in probability $|S|/n + \varepsilon$. With hight probability all the assignments mark more than $|S|$ vertices. Denote by $\tilde{S}$ the drawn vertices, with high probability $E(S,S) \le E(\tilde{S}, \tilde{S})$, just by monotonically. On the other hand:  
\begin{equation*}
  \begin{split}
    \expp{E(S, S)} & \le \left( 1 - o(\star) \right)  \expp{E(\tilde{S}, \tilde{S})} \le \left( 1 - o(\star) \right) \sum_{i\sim j} A_{ij}\expp{x_{i}x_{j}}  \approx \left( \frac{|S|}{n } + \varepsilon \right)^{2} \cdot \Delta n \\
    & \le \frac{|S|}{n}\left( 1 + \frac{\varepsilon n }{|S|} \right)^{2} \cdot \Delta |S|
  \end{split}
\end{equation*}
In particular if $|S|/ n \le \frac{\delta}{\Delta} $ then: 
\begin{equation*}
  \begin{split}
    \expp{E(S, S)} \approx \le (1 + \varepsilon \frac{n}{|S|} )^{2}  \delta |S|
  \end{split}
\end{equation*}
Concentration in the random picking is given by: 
\href{https://math.stackexchange.com/questions/3465043/concentration-of-the-number-of-edges-inside-a-random-induced-subgraph}{ Math-exchange concentration.}
\href{https://www.cs.columbia.edu/~djhsu/coms4773-s24/lectures/mcdiarmid.pdf}{Mcdiarmid}

Then we need to show that montonic of random implies montonic of concentrations:
\begin{equation*}
  \begin{split}
    \prb{E(S,S) \ge \xi } = \sum_{i \ge \xi}\prb{E(S,S) = i} \le \sum_{i \ge \xi}\prb{E(\tilde{S},\tilde{S}) = i} \le e^{-\Theta(n) }
  \end{split}
\end{equation*}
\ctt{ The concentration holds if $|S| = \Theta(n)$, However that is going to case as in every time tick we obserb more $\sim pn$ noise. }

\section{Change the basic Assumption.} 
Idea, instead of preserving a local stochastic noise, maybe preserving a monotonic $\le$ Bernoulli, would be better? The specific statement is: 

\begin{definition}
  Let $\omega \in \Omega$ be an atomic event, then we would like to have that: $q(\omega) \le \mathbf{B}_{p}(\omega)$. 
\end{definition}


\end{document}
