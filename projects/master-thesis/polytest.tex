
\newcommand{\FF}{\mathbb{F}_{q}}
\newcommand{\VV}{\sum_{t}{V(x + w^{t}y)}}
%Denote by $V(x)$ the vandermonde matrix defined by $x$ namely $V(x)_{ij} = x_{i}^{j}$. and by $w$ the $d$ root of $1$. 
%
%
%\begin{claim} 
%  \label{claim:van }
%Now $\sum_{t}{ V\left( x +  w^{t}y \right)  } = t \cdot \left(V(x) + V(y) \right)$.   
%\end{claim}
%
%\begin{equation*}
%  \begin{split}
%  \left(\sum_{t}V(x + w^{t}y)\right)_{j} = \sum{ \sum w^{l\cdot t} { j \choose l }x_{i}^{l}y_{i}^{j - l}   } = t \left( \cdot x^{j} + y^{j}\right) + \sum_{l\neq 0,j}{\frac{w^{l\cdot d} - 1 }{w^{l}-1}\cdot {j \choose l}x_{i}^{l}y_{i}^{j-l}}
%  \end{split}
%\end{equation*}
%
\begin{claim}
  Consider the finte field $\FF$ for prime $q$, denote by $w$ the $d$ root of $1$, namely $w^{d}  = 1$ then for any polynomial $f$ with degree at most $d$ it holds:   
  \begin{equation}
    \label{eq:poly}
    f(x) = \frac{1}{d} \sum_{t}^{d}{f( x + w^{t}y)}  \ \ \ \forall x,y \in \FF
  \end{equation} 
\end{claim}
\begin{proof}
  Denote by $a_{j}$ the $j$th coffecicient of $f$. And recall that for any fiexd $l \in \FF$ the summtiom $ \sum_{t}^{d}w^{t\cdot l}$ equals: 

  \begin{equation*}
    \sum_{t}^{d}w^{t\cdot l} = \begin{cases}
      \frac{w^{l \cdot d} - 1}{ w^{l} - 1 } = 0  & l \neq 0 \\
      d   & l = 0
    \end{cases}
  \end{equation*}
Thus:  
  \begin{equation*}
    \begin{split}
  &  \sum_{t}{ f\left( x + w^{t}y \right)  } = \sum_{t,j}{a_{j}\left( x + w^{t}y \right)^{j}}  =  \sum_{t,j}{ a_{j} \left( \sum_{l} x^{j-l} w^{t\cdot l}y^{l} { j \choose l} \right)} \\
  & =  \sum_{j}{ a_{j} \sum_{l} x^{j-l} y^{l} { j \choose l} \sum_{t} w^{t\cdot l} } \\
  & = \sum_{j}{ a_{j}  x^{j} \cdot d } = f(x) \cdot d  
    \end{split}
  \end{equation*}
\end{proof}
For example consider a linear function over $\FF$, then it $w^2 = 1 \Rightarrow w = -1 $ and it clears that $f(x) = \frac{1}{2} \left( f(x - y) + f(x + y) \right)$.

\begin{claim}
  Suppose that $f : \FF \rightarrow \FF$ satisfies \cref{eq:poly} than, ethier $f$ is polyonomial at degree at most $d$ or that $f$ is the zero function.  
\end{claim}
\begin{proof}
  Assume by contradiction that $f$ has more then $d+1$ zeros (w.l.g). Denote them  by $x_{1},x_{2}.. x_{d+1}$. We will show that for any $x \in \FF $ one can find $y \in \FF$ such that $x + w^{t}y =x_{t}$. Those equations compose a linear diophantine system which can reduced to the following system: 
  \begin{equation*}  
\begin{bmatrix}
  w^{0}                     & 0 &  \dots                   &  0 &     0                 \\
                               w^{1}                & 1     &  \dots             & 0      &       0                    \\
                              \vdots                    &  \ddots               &  \vdots                    \\
                               w^{d-1}              & 0    &  \dots             &   1   &       0                    \\
                               w^{d}              & 0 & \dots       &  0     &       1
\end{bmatrix} 
\begin{bmatrix}
  y \\ 
  k_{2} \\
  k_{3} \\
  \vdots  \\
  k_{d} \\  
\end{bmatrix}
= \begin{bmatrix}
  x_{1} - x \\
  x_{2} -x \\
  \vdots \\
  x_{d} - x \\
  x_{d+1} - x
\end{bmatrix}
  \end{equation*}
  The determinant of the above matrix is $w \neq 0$ and therefore there exists a $y$ satisfies the equations. Hence for any $x$ we have $f(x) = \frac{1}{d}\left( f(x_{1}) + f(x_{2}) + .. + f(x_{d+1}) \right) = 0$.   
\end{proof}
Consider the following decoding process. Given a candidate $f : \FF \rightarrow \FF$, we will set for any point $x \in \FF$ the plurality of \cref{eq:poly} over the $y \in \FF$. Denote the output by $g$, and we have that:
\begin{equation*}
  \begin{split}
    g(x) = \arg_{z\in \FF} \max \prbm{ \sum_{i}^{d}{f\left( x + w^{i}y \right) = z }}{y}
  \end{split}
\end{equation*}

\begin{claim}
  If $f$ pass the test with probability $\ge 1 - \varepsilon$ then the relative distance between $f$ and $g$ is at most $2\varepsilon$.
\end{claim}

\begin{proof}
  Notice that for any $x$ for  which there exists $z$ such $\prbm{ \sum_{i}^{d}{f\left( x + w^{i}y \right) = z }}{y} \ge \frac{1}{2}$ it must hold that $g(x) = f(x)$. Denote by $A$ that set and by $\alpha$ the probability to hit such $x$ (namely $|A| / |\FF|$. The probability to pass the test can be bounded by:  
  \begin{equation*}
    \begin{split}
      1- \varepsilon & \le \prb{ f \text{ pass}} = \prb{ f \text{ pass} \cap x \in A } + \prb{ f \text{ pass} \cap x \not\in A} \\
      & \le 1 \cdot \alpha + \frac{1}{2} \cdot \left( 1 -\alpha \right) \\
      \Rightarrow & \alpha \ge 1 - 2\varepsilon 
    \end{split}
  \end{equation*}
  Thus, at most $2\varepsilon$ of the points do not agree with $g$. 
\end{proof}

\begin{claim}
  Let $f$ be a function that pass the test with probability $1-\varepsilon$ and fix $x \in \mathbb{F}$ then: 
\begin{equation*}
  \begin{split}
    \prbm{  \sum_{i}^{d}{f\left( x + w^{i}u \right)} = \sum_{i}^{d}{f\left( x + w^{i}v \right)} }{ u,v} \ge 1 - 2\varepsilon \cdot d
  \end{split}
\end{equation*}
\end{claim}

\begin{proof}
  Recall that for any $i$ the random variabile $x + w^{i}u$ is distributed uniformaly random regardless $x$. Therefore we have that for any $i$ the following equation holds with probability at least $1 -\varepsilon$:      
  \begin{equation*}
    \begin{split}
    f\left( x + w^{i}u \right) = \sum_{j}^{d}{f\left( x +w^{i}u + w^{j}v \right)} 
    \end{split}
  \end{equation*}
  Thus, by the union bound, those equastions satisfied simulatnusly for all the $i$ with probability $1 - d\cdot \varepsilon$. By reappting on the same arguments but swapping $u$ and $v$ we have that 
\begin{equation*}
    \begin{split}
    f\left( x + w^{i}v \right) = \sum_{j}^{d}{f\left( x +w^{i}v + w^{j}u \right)} 
    \end{split}
  \end{equation*}
also happens with probability at least $1- d\cdot\varepsilon$ and therefore, again by the union bound, both of the equations sets hold with probability at least $1 - 2d\cdot\varepsilon$. But that is exactly the event in which: 
\begin{equation*}
  \begin{split} 
    \sum_{i}{f\left( x + w^{i}u \right)} = \sum_{i}^{d}\sum^{d}_{j}{f\left( x +w^{i}u + w^{j}v \right)} =  \sum_{j}{f\left( x + w^{j}v \right)}
  \end{split}
\end{equation*}
\end{proof}
\begin{claim}
  \label{claim:fixx}
  For any $x$ the probability over $v$ that $g\left( x \right) = \sum_{i}{f(x + w^{i}v)}$ is grater than $ 1 - \Theta\left( d \cdot \varepsilon  \right)$. 
\end{claim}
\begin{proof}
  Fix arbitrary $x$ and denote by $\alpha$ the probability to hit $v$ such that $g(x) = \sum_{i}^{d}{f\left( x + w^{i}v \right)}$ and by $A$ the set of $v$'s satisfy the relation. Notice that the probabiliy to pick a pair $v,u$ such that $ \sum_{i}^{d}{f\left( x + w^{i}v \right)} = \sum_{i}^{d}{f\left( x + w^{i}u \right)}$ is lower than the prbability that either $v$ and $u$ are both belong to $A$ or they both belong to his complentarty. Namely: $ \alpha^{2} + (1-\alpha)^{2}\ge 1- 2 \varepsilon\cdot d$. Hence we get $\alpha \ge \frac{1}{2}\left( 1 + \sqrt{1 - 4\varepsilon d } \right) = 1 - \Theta\left( \varepsilon \cdot d \right)$.  
\end{proof}

\begin{claim}
  For $\varepsilon = \Theta(1/d^{2})$ $g$ is a polynomial at degree at most $d$. 
\end{claim}

\begin{proof}
  Fix $x,y \in \FF$. From \cref{claim:fixx} with probability at least $1 - \Theta \left( d\cdot \varepsilon \right)$ it holds that $ g\left( x + w^{j}y \right) = \sum_{j}^{d}{f\left( x + w^{i}v + w^{j}y \right)}$. Moreover, as $ v+w^{j}y$ distributed uniformly for any $j$, it follows that one can fix $j$ and has with probability at least $1 - \Theta\left( \varepsilon d  \right)$ that  $g\left( x\right) = \sum_{i}^{d}{\left( f\left( x + w^{i}\left( v + w^{j}y \right) \right) \right)}$. So, the raltion holds for all the $j$'s with probability $1 - \Theta\left( \varepsilon d^{2} \right)$ combining all togeter obtains:  
  \begin{equation*}
    \label{eq:eqproof}
    \begin{split}
      g\left( x \right) &= \sum_{i,j}^{d}{f\left( x + w^{i}\left( v +w^{j}y\right) \right)} =  \sum_{j}^{d}\sum_{i}^{d}{f\left( x + w^{i}v + w^{j}y \right)} = \sum_{j}^{d}{g\left( x + w^{j}y \right)} 
    \end{split}
  \end{equation*}
  With probability $1 - \Theta \left( \varepsilon \cdot d^{2}\right)$. However, the probability is about sampling $v$ such those transimitions are vaild. But as the value of $g$ is indepanded on the choice of $v$ it's enough to have just a single $v$ to garuntee $g\left( x \right) = \sum_{j}^{d}{g\left( x + w^{j}y \right)}$ namely that the probability is positive. So if  $ \varepsilon = \Theta(1/d^{2}) $ we have that for any $x,y$ there exists $v$ such \cref{eq:eqproof} is satisfied and therfore $g$ is satisfies \cref{eq:poly} for any $x,y$. 
\end{proof}



We are going to introduce the polynomial code in an opposite order as in the usual literature. Instead of starting by presenting polynomials and the code itself we will begin by defining an abstract decoder, and define the code to be all the strings on which the decoder is apathetic. Consider the alphabet $\Sigma$, and let $D$ be a decoder such that on every $d+1$ coordinate $x_1,x_2, .. x_{d+1}$ and candidate for a code word $c$, read $c_1, c_2 .. c_{d}$ and returns a charter in $\Sigma$. We will think of $D$ as tester also. On given candidate $c \in \Sigma^{n}$, $D$ accept if for any $x_1,x_2.. x_{d+1}$ it holds that $C_{d+1} = D\left(c; x_1, x_2 .. x_{d+1} \right)$. We will associciate a single check with spesific $d+1$ coordinates.   

The setting avbove is kind general, and we will need to rquire form the code more to be a locally testable. Let us define the following property which could be thought as induction for cheks degenration:

\begin{definition}
  We will say that code $C$ has the \textit{degeneration} property if for any chack $h$ there is a set of checks $H\left( h \right)$ such that: 
  \begin{enumerate}
    \item if $H$ satisfied then also $h$ satisifed. 
    \item it easy to chack $H$.  
  \end{enumerate} 
\end{definition}

 

\begin{claim}The code defined by all the strings on which $D$ accept with probability $1$, when the randomness is over the sampled coordinates $x_{1}, x_{2} .. x_{d+1}$ is a locally testable code. In particular, if the $c$ is in the code, then a random check successes with probability $1$ and for every $c$ that is $\varepsilon$-far from the code the probability to, accept  is at most $ { d \choose 2 } |\Sigma| \varepsilon $.  
\end{claim}

\begin{proof}
  Notice that by definition if the tester accept with probability $1$ then $c \in C$. Suppose that $c$ pass the test with probability at least $1 - \varepsilon$. First, let us define the codeword $l$ such that $ l_{y} = Majoritiy_{x_1 .. x_{d}}D\left(c, x_{1}.. x_{d},y\right)$.

  \begin{claim} 
    The distance between $l$ and $c$ is at most $ 1- \left( 1 - \frac{1}{\Sigma} \right)^{-1}\varepsilon$.  
  \end{claim}
  \begin{proof} 
    Denote by $\alpha$ the probability that for given $y$, $D$ accept on more then $\frac{1}{|\Sigma|}$ fraction to choose $x_{1},x_{2}..x_{d}$. 
    Then we have that:  
    
    \begin{equation*}
      \begin{split}
        1 - \varepsilon  & \le \prbm{ D\left( x_1 .. x_{d+1} \right) }{x_1 .. x_{d+1}}  \\
        & = \prbcprb{  D\left( x_1 .. x_{d+1} \right)}{ x_1 .. x_{d+1}  }{ x_{d+1} \in A } + \\
        & \ \ \ \ \ \ \prbcprb{ D\left( x_1 .. x_{d+1} \right)}{ x_1 .. x_{d+1}   }{ x_{d+1} \notin A } \\
        & \le 1 \cdot \alpha +  \frac{1}{|\Sigma|}  \left( 1 - \alpha \right) = \left( 1 - \frac{1}{|\Sigma|} \right) \alpha  + \frac{1}{|\Sigma|} \\
       & \Rightarrow \alpha \ge  1- \left( 1 - \frac{1}{\Sigma} \right)^{-1}\varepsilon
      \end{split}
    \end{equation*}
  \end{proof} 

  So we showed that $l$ and $c$ are close. It's left to show that $l$ is a code word, namely that any check by $D$ pass. Fix a check, which equivalence as fix $d+1$ coordinates $x_{1} .. x_{d+1}$. Now notice that by the degenration property if there exists $y$ such that for every $i \in [d+1]$ $D\left(l; y_{1} .. y_{d}, x_{i} \right) = l_{x_{i}}$ than it follows that also $D\left(l; x_{1} .. x_{d+1} \right)$ pass.  
  \begin{equation*}
    \begin{split}
      \left\{ D(G(c) ; x ) \right\} \subset \bigcap_{ x_{i} } \left\{ D\left( x_{i} ; y_{1} .. y_{d} \right)  \right\} 
    \end{split}
  \end{equation*}
\end{proof}


