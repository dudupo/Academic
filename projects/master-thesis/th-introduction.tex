\chapter{Introduction}
Many experts believe that quantum computing is a highly effective computation model. The consensus is that certain problems that are difficult for classical computers to solve can be effectively solved with quantum computers. One of the biggest challenges in simulation is chemical simulation. Although we have the ability to simulate the workings of electrical circuits, the durability of structures, and the flight of aircrafts, simulating the evolution of an 80-atom molecule over time is still beyond our capabilities, even with the assistance of the world's most powerful supercomputer. When it comes to simulating molecules, the classical method can be inefficient as it requires tracking a large number of variables in memory. However, quantum hardware, based on the principles of quantum mechanics, allows for a more efficient encoding of the molecule's structure, mimicking the way it exists in nature. Although it is unclear how to prove that classical algorithms cannot efficiently simulate molecules, we do know that quantum simulation is easily achievable and uses memory resources efficiently. Therefore, experts envision that in the future, drug development and materials engineering will become more systematic. Today, researchers proposing candidates for drugs or vaccines may have to wait a long time for approval, but in the future, simulations will provide real-time answers on a screen. This will be similar to the ease with which people share libraries of code or hardware, establish companies, and release products. As a result, we will see a complex and interconnected industry dealing with the development of drugs, materials, genetic engineering, and more.

Despite the undeniable superiority of the quantum computation model over the classical model, there remains a significant challenge: the implementation of the quantum computation model. The essential components of quantum hardware are prone to numerous errors, and currently, no quantum computers can overcome these errors. Even a slightly lengthy computation accumulates enough errors to render the final computation result incomprehensible. This issue is reminiscent of a problem faced by Von Neumann, one of the inventors of the classical computer, nearly a century ago. He asked if classical computation was possible despite the noise and answered affirmatively. It is worth noting that hardware has advanced considerably since then, but most of the ideas of "fault tolerance computation" have yet to be realized. However, many ideas are critical components of everyday products, such as the LDPC codes used for error correction in cellular communication in the G5 protocol. Researchers have also demonstrated that classical ideas can be generalized to quantum cases to enable "quantum fault tolerance computation." The ideas are based on a unique code encoding system that facilitates easy identification and correction of errors.

In addition to error correction codes, there are also codes that designed to be tested effectively. To understand what testable codes are,  Imagine in your head a puzzle in which all the pieces are painted in exactly the same color. Such a puzzle will be called untestable if there is a placement of the pieces of the puzzle so that on the one hand it is worthwhile to reach it, half of the pieces must be swapped for obtaining  the "true placement" of the puzzle - that is, the correct way to put it together - but on the other hand there are a few discrepancies between the connections of the different parts. That is, you have to work a lot to get to the "real placement," but if you sample an arbitrary connection, there is zero chance that a contradiction will appear. Testable puzzles are the exact opposite; if a placement is very far from the "true placement," then an arbitrary test will most likely reveal it. The existence of good quantum codes and efficiently testable classical codes have been open questions for a long time. Although, it seems like, there is no direct connection between them, both have been obtained by the same one advanced construction that was developed only two years ago (2021). Hence, it is interesting to ask if this is a coincidence or if there is a deeper connection between the two that we no longer understand.

In this work, we review, end to end, the quantum error correction codes. Starting from an overview of classical codes, methods for constructing good codes, characterization of quantum noise and primitive quantum codes, to the advanced construction of good LDPC quantum codes. All this is alongside a review of testable codes. Finally, we present failed attempts in follow-up research, in particular, an attempt to reach the construction of testable classical codes without simultaneously developing good quantum codes. If we were able to isolate the results, it would indicate that the fact that both were obtained in the same construction is completely coincidental. At the same time, we haven't progressed far enough to suspect that there is a necessary connection reasonably.

The work assumes only a basic knowledge of linear algebra and combinatorics. So we believe that every computer science graduate will be able to enjoy reading it, understand the subject very well, and use it as a gateway for starting research in the field.

%The PCP theorem states that there exists a class of computational problems that can be probabilistically verified with high accuracy by reading only a constant number of bits from a proof that is polynomial in the input size of the problem. This means that given a proof for a problem, a verifier can check the proof for correctness by reading only a few bits of the proof, making the verification process efficient.

%A deterministic test for comparing two numbers can be done by simply comparing each digit in the numbers one by one. 
%
%The coordinate picking test is a probabilistic test for comparing the equality of two numbers. It's efficient because it provides a high probability of correctly determining if two numbers are different, especially if the numbers are far apart from each other in terms of their digits.
%
%To use the coordinate picking test, a random digit from each number is chosen, and the test checks if the chosen digits from both numbers are the same or different. The process is repeated for multiple positions or coordinates, and if all the chosen coordinates match, the numbers are considered equal.
%
%When the numbers being compared are far apart in terms of their digits, the probability of detecting their difference with the coordinate picking test becomes higher. This is because it's more likely to find a position where the numbers differ upon choosing random coordinates for comparison.
%
%Overall, the coordinate picking test is an efficient probabilistic algorithm for comparing the equality of two numbers, especially if the numbers are far apart in terms of their digits.
%
%The quantum swap test is an algorithm used in quantum computing to compare the similarity of two large strings of information. It works by manipulating qubits to determine whether two large strings are identical or different. The swap test can be used on product states, which are tensor products of individual qubits. 
%
%To use the quantum swap test, two large strings of information (in quantum form) are prepared, and the algorithm compares them by swapping qubits and measuring the resulting state. The process is repeated multiple times to improve the accuracy of the test. 
%
%In contrast to the probabilistic coordinate picking test, the quantum swap test is a deterministic algorithm that can determine the equality of two numbers with high probability. However, the quantum swap test requires complex quantum operations that may be difficult to implement, and it may not be efficient for very large inputs.
%
%Regarding a hypothetical test where a random coordinate is picked and only the corresponding qubits are swapped, such a test would likely fail with high probability in mimicking the coordinate picking test, as it would not take into account the correlation between qubits that exist in quantum states. Unlike classical bits, qubits can exist in superpositions, which means they can represent multiple states simultaneously. Therefore, a test that only swaps particular qubits without taking into account their entanglement is unlikely to give accurate results in a way that mimics the classical coordinate picking test.
%
%The PCP (Probabilistically Checkable Proof) theorem is a fundamental result in theoretical computer science that has broad implications in various fields, including computational complexity theory, cryptography, and algorithms design. It describes the relationship between the amount of information required to verify a proof of a computational problem and the computational resources needed to find a solution to that problem.
%
%%
%A deterministic test for comparing two numbers can be done by simply comparing each digit in the numbers one by one; however, the coordinate picking test is a probabilistic test that provides a high probability of correctly determining if two numbers are different, especially if the numbers are far apart from each other in terms of their digits. In contrast, the quantum swap test is a deterministic algorithm that can determine the equality of two numbers with high probability, but it requires complex quantum operations and may not be efficient for very large inputs. Moreover, a hypothetical test where a random coordinate is picked and only the corresponding qubits are swapped is unlikely to give accurate results in a way that mimics the classical coordinate picking test. Finally, the PCP theorem states that there exists a class of computational problems that can be probabilistically verified with high accuracy by reading only a constant number of bits from a proof that is polynomial in the input size of the problem, making the verification process efficient.
%The theorem also shows that there exists a connection between the resources required to verify a proof and the computational resources needed to solve the corresponding computational problem. Specifically, the theorem states that if a problem can be verified with high accuracy using a small amount of proof information, then the problem can be solved efficiently using a probabilistic algorithm. This provides a powerful tool for studying the computational complexity of problems, as it allows us to focus on verifying solutions rather than finding them.
%
%The PCP theorem has numerous implications in various fields, such as applications in cryptography and the potential to improve algorithm design and optimization. It has also led to the development of new techniques for solving computational problems efficiently, such as the use of probabilistically checkable proofs in conjunction with the power of randomized algorithms.
%
%In summary, the PCP theorem provides a fundamental insight into the relationship between proof verification and computational complexity, and has implications for numerous fields of study. It has led to the development of new techniques and algorithms for solving complex problems efficiently, and offers a powerful tool for understanding the complexity of computational problems.
%

%\printbibliography[heading=subbibliography]
